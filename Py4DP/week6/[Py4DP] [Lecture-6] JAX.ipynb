{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "plt.style.use(\"bmh\")\n",
    "plt.rcParams[\"figure.figsize\"] = (6,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install JAX on CPU, use:\n",
    "    \n",
    "```\n",
    "pip3 install jax[cpu]\n",
    "```\n",
    "\n",
    "To install JAX with GPU support, refer to [documentation](https://github.com/google/jax#installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(1000, centers=[[-3, -3], [0, 0]], cluster_std=1.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6, edgecolor='k',\n",
    "            cmap=plt.cm.coolwarm, vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x_0$', fontsize=14)\n",
    "plt.ylabel('$x_1$', fontsize=14)\n",
    "plt.title(\"Actual targets\", fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xj = jnp.array(X)\n",
    "yj = jnp.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic operations\n",
    "\n",
    "Random numbers generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(0)\n",
    "\n",
    "W = random.normal(key, (2, 1))\n",
    "b = random.normal(key, (1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear algebra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.dot(Xj, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "\n",
    "    return 1/(1 + jnp.exp(-a))\n",
    "\n",
    "def regressor(x, w, b):\n",
    "    \"\"\"Full logistic regression expression.\"\"\"\n",
    "    return sigmoid(jnp.dot(x, w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor(Xj, W, b)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred.flatten(), alpha=0.6, edgecolor='k',\n",
    "            cmap=plt.cm.coolwarm, vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x_0$', fontsize=14)\n",
    "plt.ylabel('$x_1$', fontsize=14)\n",
    "plt.plot([0, W[0, 0].item()],[0, W[1,0].item()], \"-\", c=\"forestgreen\", linewidth=4)\n",
    "plt.title(\"Predicted targets\", fontsize=12)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6, edgecolor='k',\n",
    "            cmap=plt.cm.coolwarm, vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x_0$', fontsize=14)\n",
    "plt.ylabel('$x_1$', fontsize=14)\n",
    "plt.title(\"Actual targets\", fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIT compilation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_jit = jit(regressor)\n",
    "regressor_jit(Xj, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call are async by default (doesn't matter that much for CPU, but is extremely important for GPU operations):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit regressor_jit(Xj, W, b).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit regressor(Xj, W, b).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit np.dot(X, W.to_py())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "Wouldn't it be great to have automatic vectorization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wf = random.normal(key, (2,))\n",
    "bf = random.normal(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wf, bf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single example operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(Xj[0] * Wf).sum() + bf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression function applicable to a single element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "\n",
    "    return 1/(1 + jnp.exp(-a))\n",
    "\n",
    "def regressor(x, w, b):\n",
    "    \"\"\"Full logistic regression expression.\"\"\"\n",
    "    return sigmoid(jnp.sum(x * w) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor(Xj[0], Wf, bf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization is done with `vmap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_batch = vmap(regressor, in_axes=(0, None, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_batch(Xj, Wf, bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 10 -r 3 regressor_batch(Xj, Wf, bf).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIT is composable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_batch_jit = jit(regressor_batch)\n",
    "regressor_batch_jit(Xj, Wf, bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit regressor_batch_jit(Xj, Wf, bf).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit regressor_batch(Xj, Wf, bf).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autodiff\n",
    "\n",
    "Autodiff in JAX is functional, and is applied explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_grad = grad(sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_grad(0.1), sigmoid(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_grad(0.1), sigmoid(0.1) * (1 - sigmoid(0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return sigmoid(x) * sigmoid(2 * y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XVAL = 0.1\n",
    "YVAL = 0.1\n",
    "\n",
    "f_grad = grad(f, argnums=(0, 1))\n",
    "f_grad(XVAL, YVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(XVAL) * sigmoid(2 * YVAL) * (1 - sigmoid(XVAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2 * sigmoid(XVAL) * sigmoid(2 * YVAL) * (1 - sigmoid(2 * YVAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def loss(X, Y, w, b):\n",
    "    \"\"\"Loss function suitable for JAX autodiff.\"\"\"\n",
    "\n",
    "    y_pred = regressor_batch(X, w, b)\n",
    "    return -jnp.mean(Y * jnp.log(y_pred) + (1 - Y) * jnp.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad = jit(grad(loss, argnums=(2,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wf = random.normal(key, (2,))\n",
    "bf = random.normal(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad(Xj, yj, Wf, bf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000\n",
    "LR = 1e-1\n",
    "DELTA = 0.00001\n",
    "loss_history = []\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    current_loss = loss(Xj, yj, Wf, bf)\n",
    "    w_grad, b_grad = loss_grad(Xj, yj, Wf, bf)\n",
    "    loss_history.append(current_loss.to_py())\n",
    "\n",
    "    Wf = Wf - w_grad * LR\n",
    "    bf = bf - b_grad * LR\n",
    "\n",
    "    if i % 20 == 0:\n",
    "        print(f\"Epoch {i}: loss = {loss_history[-1]}\")\n",
    "    \n",
    "    try:\n",
    "        if loss_history[-2] - loss_history[-1] < DELTA:\n",
    "            break\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor_batch(Xj, Wf, bf)\n",
    "\n",
    "plt.figure(figsize=(14,6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred.flatten(), alpha=0.6, edgecolor='k',\n",
    "            cmap=plt.cm.coolwarm, vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x_0$', fontsize=14)\n",
    "plt.ylabel('$x_1$', fontsize=14)\n",
    "plt.plot([0, Wf[0].item()],\n",
    "         [0, W[1].item()],\n",
    "         \"-\",\n",
    "         c=\"forestgreen\",\n",
    "         linewidth=4)\n",
    "plt.title(\"Predicted targets\", fontsize=12)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.6, edgecolor='k',\n",
    "            cmap=plt.cm.coolwarm, vmin=0, vmax=1)\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x_0$', fontsize=14)\n",
    "plt.ylabel('$x_1$', fontsize=14)\n",
    "plt.title(\"Actual targets\", fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wf, bf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class = (y_pred >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y, y_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_pred[y==0], range=(0,1))\n",
    "plt.hist(y_pred[y==1], range=(0,1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnp.mean(y_class == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
