{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGDl7CwR81y2"
   },
   "source": [
    "# PyTorch handling of GPU devices\n",
    "\n",
    "Generally, PyTorch is pretty transparent about how it handles devices. When using some high-level wrappers (like [PyTorch Lightning](https://pytorch-lightning.readthedocs.io/)) you do not need to do much work to transfer a deep learning model (and train it) to a GPU.\n",
    "\n",
    "However, it's beneficial to have some basic understanding of how GPUs work, know some basic terminology and understand how to move things around manually. As a bonus, this notebook also brings some very basic GPGPU understanding.\n",
    "\n",
    "# Parallel linear algebra\n",
    "\n",
    "**G**raphical **P**rocessing **U**nit is a highly parallel type of computing accelerator. It can handle thousands of parallel computation in **data-parallel** manner. That being said, we need to clarify why is it important to have such a capability.\n",
    "\n",
    "Deep learning models intrinsically perform linear algebra operations of various sorts.\n",
    "\n",
    "For example, **feed-forward neural nets** (the one you had in HW3 was exactly that) contain matrix-vector multiplications. **Convolutional neural networks** contain a lot of **convolutions**.\n",
    "\n",
    "Hence, in all practical occasions, we need to deal with vectors, matrices, and more-than-2-dimensional objects (generally called *arrays* in NumPy and *tensors* in PyTorch). The main property of such operations is that they are usually quite easy to parallelize (at least it is easy *conceptually*, although actual implementation may be quite complex).\n",
    "\n",
    "Consider, for example, matrix-vector product:\n",
    "\n",
    "$$u_i = A_{ij}v_j.$$\n",
    "\n",
    "Or, rewriting it in a more explicit form:\n",
    "\n",
    "$$u_i = \\sum_j A_{ij}v_j = A_{i,0}v_0 + A_{i,1}v_1 + \\cdots + A_{i,N-1}v_{N-1}.$$\n",
    "\n",
    "Each component of $\\vec u$ can be calculated independently of all the others. You just need to take **0-th row of $A$** (which is a vector on its own), calculate its **element-wise product** with $\\vec v$, **reduce** the result with $+$ and here's the **0-th component** of $\\vec u$. There's no reason why you cannot calculate 1-st (or 2-d, or etc.) component at the same time.\n",
    "\n",
    "Ideally, given $N\\times M$ matrix $A$, and $M\\times 1$ vector $\\vec v$, parallel computation should bring a speed-up of $N$, since all $N$ components of $\\vec u$ are calculated in parallel. It is not that simple in reality due to many technical reasons, but for simple calculations it's almost true. In general, the actual speed-up depends on an operation.\n",
    "\n",
    "# CPU vs. GPU\n",
    "\n",
    "CPU will perform such operations with a very little (at least compared to GPU) level of parallelism. Yes, CPUs have multiple cores and vector extensions (like SSE or AVX), which allow to make this process somewhat parallel and utilize hardware to its full capacity.\n",
    "\n",
    "But the main thing is that CPUs were designed to do various things - basically, everything imaginable. GPUs, in contrast, were designed to do one thing (compute) and do that thing very well and with a tremendous amount of parallelism (since graphics computations are intrinsically parallel, thank you, gamers!).\n",
    "\n",
    "# CUDA\n",
    "\n",
    "It was 2007 when NVIDIA introduced **CUDA** (**C**ompute **U**nified **D**evice **A**rchitecture) - first usable framework for **g**eneral-**p**urpose computing on **g**raphics **p**rocessing **u**nits (GPGPU). Before then, researchers used computing power of GPUs through smart trickeries over OpenGL shading language. The procedure was error-prone and extremely inconvenient. CUDA simplified all of that dramatically.\n",
    "\n",
    "At the same time, CUDA was exactly what was needed for deep learning and led to outstanding progress in the field. Initially, you had to do everything in C (with some CUDA-specific extensions), but very quickly wrappers for Python and other programming languages came to market. Now you can use Python (through general-purpose Numba, or deep learning specific PyTorch and Tensorflow), Julia (through `CUDAnative.jl`, `CuArrays.jl` and other packages) and other languages.\n",
    "\n",
    "It's important to understand that CUDA works only with NVIDIA GPUs (de-facto standard for GPGPU in general and deep learning in particular). Neither PyTorch, nor Tensorflow can handle other GPUs as well (there are efforts in the direction of using other GPU devices, the ones from AMD or Intel GPU cores, but it's very limited). This happened because NVIDIA treated CUDA as a first-class citizen and not as a byproduct of gaming and graphical applications.\n",
    "\n",
    "\n",
    "# GPUs and CUDA programming model\n",
    "\n",
    "*The following is a basic introduction to GPGPU with CUDA. If you're only interested in reading about PyTorch on GPUs, you can skip to the next section.*\n",
    "\n",
    "CUDA programming model is very simple and straightforward. GPU chips:\n",
    "\n",
    "- are comprised of **multiprocessors** (MP),\n",
    "- have their own **global memory**,\n",
    "- each multiprocessor, in turn, is comprised of a number of computing **cores** (cores are different and exact composition depends on the device),\n",
    "- and has its own **shared memory**, available to all computing cores.\n",
    "\n",
    "Those are main resources to consider when launching a computing **kernel**, i.e. a function, which will run on a GPU.\n",
    "\n",
    "CUDA programming model reflects hardware architecture almost one-to-one:\n",
    "- kernels are launched on a grid of **blocks**,\n",
    "- each block contains a grid of **threads**,\n",
    "- a block will run on one multiprocessor,\n",
    "- a thread will run on one core,\n",
    "- all threads have access to global memory,\n",
    "- all threads *inside a block* have access to multiprocessor's shared memory.\n",
    "\n",
    "Hence, it's a direct hardware to software correspondence: **multiprocessor** $\\rightarrow$ **block**, **core** $\\rightarrow$ **thread**.\n",
    "\n",
    "But what does it mean to launch a *grid* of something? Let's consider an example. Imagine, that you want to add two vectors $\\vec a$ and $\\vec b$ of shape $N$ on a GPU (too simple thing to do on a GPU, but simple enough to explain the idea).\n",
    "\n",
    "Our basic execution unit is a thread. A thread will take some component, say $i$, from each vector and add them together. Since our problem is intrinsically 1-dimensional, we will launch a $1-D$ grid of blocks, each containing a $1-D$ grid of threads. How many blocks should we have? GPUs have some limitations. The first to consider is **maximum number of threads per block**.\n",
    "\n",
    "Below I use Julia for simplicity (syntax is self-evident and Julia REPL is the fastest way to do this, although we will use Numba and PyTorch later on, as this is the Python class, not Julia) and run this on my machine:\n",
    "\n",
    "```julia\n",
    "julia> using CUDA\n",
    "julia> dev = CUDA.CuDevice(0)\n",
    "CuDevice(0): GeForce GTX 1650\n",
    "\n",
    "julia> CUDA.attribute(dev, CUDA.CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT)\n",
    "16\n",
    "\n",
    "julia> CUDA.attribute(dev, CUDA.CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)\n",
    "1024\n",
    "```\n",
    "\n",
    "Ok, we may try to push as many as 1024 threads per block. How many blocks do we need? `ceil(N/1024)`, of course. Imagine, for example, that $N=5466$: we'll need 6 blocks, with 1024 threads each. It seems that we'll launch more threads than needed. It's true. But CUDA kernels operate in **data-parallel** fashion. We *do not have any loops*, it's that each thread knows where it is and takes the corresponding element from each array to add together. So, we will just add an `if` to check if thread is in bounds, .\n",
    "\n",
    "How is that possible? CUDA provides a way for each thread to calculate it's position in a grid. Four variables are available to each thread:\n",
    "\n",
    "- `blockIdx` (a C structure, having `x`, `y` and `z` field), which tells the thread to which block it belongs,\n",
    "- `threadIdx` (having a similar structure), which tells a thread where it is inside a block,\n",
    "- `gridDim` (you got, it right?), which tells the dimensions of the grid of blocks,\n",
    "- `blockDim` (...), which tells the dimensions of the grid of threads.\n",
    "\n",
    "Let's go from bottom to top: `blockDim` is `(1024, 1, 1)`, since it's $1-D$ and we want a max number of threads per block. `gridDim` is `(6, 1, 1)`, since we need 6 blocks on a `1-D` grid. Then, each thread can calculate it's absolute position as the following:\n",
    "\n",
    "```\n",
    "thread_idx = blockIdx.x * blockDim.x + threadIdx.x\n",
    "```\n",
    "\n",
    "which is guaranteed by definition to be *unique*. So, each thread should just take component `thread_idx` from each vector and add them together.\n",
    "\n",
    "Stay tuned, we're approaching the actual code for this. The last thing we need to understand is the memory hierarchy and how we `return` things (spoiler alert: we do not):\n",
    "\n",
    "- CUDA kernels **cannot operate on arrays, which are in main memory**, we need to transfer them to GPU memory first,\n",
    "- CPU-to-GPU copies are **costly** and should be minimized,\n",
    "- we'll provide **three** arrays to our GPU kernel: two operands and an array to hold the result. Yes, you cannot directly return an array from a GPU kernel (actually, you can go on with pointers, but I would not recommend you to do this).\n",
    "\n",
    "Back then, in C times, it was like this: you have CPU arrays, you allocate GPU memory, you copy from CPU memory to GPU memory (from pointer to pointer saying how many bytes you want to copy, very similar to `memcpy`), launch your kernel, copy the result back to CPU memory. With Python or Julia it's way simpler, as inner mechanics will do a lot of things for you.\n",
    "\n",
    "So, let's start. Note, that for this to run you need to have NVIDIA GPU, driver and CUDA toolkit installed. This will:\n",
    "\n",
    "- work as is in Colab without additional installations,\n",
    "- will not work in Yandex DataSphere (to my knowledge),\n",
    "- may not work properly on Windows (GPGPU is Linux-dominated). \n",
    "\n",
    "We'll use Numba, which provides convenient wrappers for CUDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cwGSLMU4_FFz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first create our arrays to add together (I'm using much larger array to see at least some performance gain):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EWY4DGde_HG5"
   },
   "outputs": [],
   "source": [
    "N = 100000\n",
    "\n",
    "a = np.random.randn(N).astype(np.float32)\n",
    "b = np.random.randn(N).astype(np.float32)\n",
    "u = np.zeros(N).astype(np.float32)\n",
    "\n",
    "u_cpu = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're using `float32`, as GPUs are somewhat less performant with `float64`. It depends on the GPU, but mine is 32 times slower on `float64` compared to `float32`.\n",
    "\n",
    "Simple enough so far. Now we need to get them on the GPU (you can skip this stage, as Numba will wrap this for you, but with a high performance overhead):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_gpu = cuda.to_device(a)\n",
    "b_gpu = cuda.to_device(b)\n",
    "u_gpu = cuda.to_device(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need 1024 threads per block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "enbQ_pW_Jn91",
    "outputId": "e803e9db-c587-46f8-902e-82d97304c57e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will launch 1024 threads in 98 blocks\n"
     ]
    }
   ],
   "source": [
    "threads_per_block = 1024\n",
    "blocks_per_grid = int(np.ceil(N / threads_per_block))\n",
    "\n",
    "print(f\"Will launch {threads_per_block} threads in {blocks_per_grid} blocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to write the kernel itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "DnnG9htZKKmT"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def cu_add_vectors(a, b, u):\n",
    "    # Getting this thread absolute position, i.e., which element this thread will calculate\n",
    "    tidx = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x # or shortcut: cuda.grid(1) \n",
    "\n",
    "    # Is this thread in bounds?\n",
    "    if tidx < a.shape[0]:\n",
    "        u[tidx] += a[tidx] + b[tidx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main thing to understand here is that we **do not write any loops** (depending on your algorithm you may need them, but not for the \"outer\" loop).\n",
    "\n",
    "Otherwise, it's pretty simple. On the first run, Numba will JIT-compile this for us, given the argument and will add copies, if operands are CPU arrays. This is not recommended, again, as copies are costly and it's better to do them once. In this case it's not important, as we anyway will have to calculate some very simple thing. Other algorithms, which have heavier computations will benefit from minimizing data transfers.\n",
    "\n",
    "We will operate on GPU array we created to measure the **computational** performance of the GPU we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cu_add_vectors[blocks_per_grid, threads_per_block](a_gpu, b_gpu, u_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our kernel is compiled and next time will run immediately. Note, how **launch configuration** is specified: `[blocks_per_grid, threads_per_block]`. Ok, now we can measure the computational performance of the GPU (our kernel is compiled, our data is on GPU already, so there's no overhead). Note, that I will skip filling `u_gpu` with `0` each time (in general you have to do that, since it accumulates the results from the previous runs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4U2BJ7O8Kn2e",
    "outputId": "ffcdd3c3-90be-4afa-e28f-18105a3d2bf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 µs ± 25.3 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit cu_add_vectors[blocks_per_grid, threads_per_block](a_gpu, b_gpu, u_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UE77azuvLHJJ",
    "outputId": "7b6f50cd-ab01-47d2-c78f-cdf8d9b5ce3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.1 µs ± 1.76 µs per loop (mean ± std. dev. of 3 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 1000 -r 3 u = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, in this simple case it's useless to use GPU - overhead on launch is too high and computations are too small. But now you understand the point and will understand the following code (which multiplies a matrix to a vector):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-dH13STLK_Ag"
   },
   "outputs": [],
   "source": [
    "N = 100000\n",
    "M = 100\n",
    "\n",
    "A = np.random.randn(N, M).astype(np.float32)\n",
    "v = np.random.randn(M).astype(np.float32)\n",
    "u = np.zeros(N).astype(np.float32)\n",
    "\n",
    "u_cpu = np.dot(A, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will launch 1024 threads in 98 blocks\n"
     ]
    }
   ],
   "source": [
    "threads_per_block = 1024\n",
    "blocks_per_grid = int(np.ceil(N / threads_per_block))\n",
    "\n",
    "print(f\"Will launch {threads_per_block} threads in {blocks_per_grid} blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def cu_mv_product(m, v, u):\n",
    "    tidx = cuda.grid(1)\n",
    "\n",
    "    # Is this thread in bounds?\n",
    "    if tidx < m.shape[0]:\n",
    "        # Creating temporary handle for the result\n",
    "        tmp  = 0.\n",
    "\n",
    "        # Doing scalar product between row `pos` in A and v\n",
    "        for i in range(m.shape[1]):\n",
    "            tmp += m[tidx, i] * v[i]\n",
    "\n",
    "        # Putting the result into u\n",
    "        u[tidx] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_gpu = cuda.to_device(A)\n",
    "v_gpu = cuda.to_device(v)\n",
    "u_gpu = cuda.to_device(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VpsjOv54Lj94"
   },
   "outputs": [],
   "source": [
    "cu_mv_product[blocks_per_grid, threads_per_block](A_gpu, v_gpu, u_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cggv7_l9LmoD",
    "outputId": "7d823db9-f1d1-4397-a4e3-484dc6c8690e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.337545 , -4.719008 ,  6.903495 , ...,  4.0871673, 10.019787 ,\n",
       "       -0.4688534], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_gpu.copy_to_host() # this will copy the data from GPU memory to host (CPU) memory and return a usual Numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaVufKeaO-hA",
    "outputId": "3b647ec1-09a7-4b12-9e20-eac4c83c9801"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15.337545 , -4.7190094,  6.903495 , ...,  4.0871677, 10.019786 ,\n",
       "       -0.4688539], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that results are slightly different (Mrs. CUDA and Mr. IEEE 754 standard is [a complicated couple](https://docs.nvidia.com/cuda/floating-point/index.html)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(u_cpu, u_gpu.copy_to_host(), atol=1e-5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we measure this one (which uses more computations), results will be very different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ozVnmJBkLAba",
    "outputId": "7f4013de-2394-4c11-8019-47d3cde54e10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264 µs ± 18.3 µs per loop (mean ± std. dev. of 3 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 -r 3 cu_mv_product[blocks_per_grid, threads_per_block](A_gpu, v_gpu, u_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.25 ms ± 64.9 µs per loop (mean ± std. dev. of 3 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 -r 3 np.dot(A, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see the difference (about `9X`). GPU was utilized properly (meaning, that percentage of launch overhead was small compared to actual computations).\n",
    "\n",
    "The last thing to cover here is a confusing situation: the actual number of MPs we have on the GPU is 16 (in my case), number of cores per MP is 32, but we launch 98 blocks with 1024 threads each. How does that work?\n",
    "\n",
    "GPU will do the following:\n",
    "\n",
    "- each block will be **assigned to an MP**,\n",
    "- each MP will get **multiple blocks to run**,\n",
    "- blocks will be **queued to run**,\n",
    "- threads will be queued to run **on cores**,\n",
    "- only 32 threads will run at the same time - a batch called **warp**.\n",
    "\n",
    "Hence, each MP will get about 6 or 7 blocks, each block will contain 32 warps (as we only have 32 cores per MP), all the warped threads will run at the same time, one per core.\n",
    "\n",
    "We do not consider *streams*, *shared memory*, *pinned memory* and other advanced topics here (like achieving best utilization, handling resources constraints and so on).\n",
    "\n",
    "If you want to get a deeper understanding, try [CUDA Programming Guide](https://docs.nvidia.com/cuda/pdf/CUDA_C_Programming_Guide.pdf): it's written in a clear and simple language. You can start with Chapters 1 and 2 and launch non-trivial code in several hours."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch and GPU\n",
    "\n",
    "If you followed the previous section, you probably have already realized that GPGPU is conceptually straightforward, but incorporates a huge number of technical details. Luckily enough, deep learning frameworks like PyTorch or Tensorflow abstract almost all of them.\n",
    "\n",
    "The main idea in training deep learning models on a GPU is about placing the tensors properly. If you recall, a PyTorch tensor always has an associated storage object. Those tensors, which live on GPU, have the corresponding storage type. That's it, for most applications that all you need to know. From a user perspective GPU tensors are used in the same way as CPU ones. All the technical complications are handled by PyTorch itself.\n",
    "\n",
    "Let's look at this further. The subpackage, which exposes some GPU convenient routines is `torch.cuda`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Srwh7_VB6tqG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we have a single GPU on my machine (you can see a different number depending on where you run this notebook). We can explicitly get GPU device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "peLHNTMU60U7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch doesn't expose deep details about GPU devices, but we can get\n",
    "\n",
    "- GPU **name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "xrpDZBe-7kss",
    "outputId": "06006597-a422-4772-eb2f-8fd33596a0d1"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14752\\2592075506.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\py4dp\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m     \"\"\"\n\u001b[1;32m--> 326\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py4dp\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \"\"\"\n\u001b[1;32m--> 356\u001b[1;33m     \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# will define _get_device_properties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py4dp\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_name(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **compute capability** (which determines MP composition and what GPU can and cannot do, see CUDA Programming guide for more details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lnp18KrN8Afa",
    "outputId": "0aa45a50-d59d-455a-8415-02b2d0d4b30a"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14752\\3971708209.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\py4dp\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mget_device_capability\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmajor\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mminor\u001b[0m \u001b[0mcuda\u001b[0m \u001b[0mcapability\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \"\"\"\n\u001b[1;32m--> 342\u001b[1;33m     \u001b[0mprop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmajor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py4dp\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \"\"\"\n\u001b[1;32m--> 356\u001b[1;33m     \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# will define _get_device_properties\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py4dp\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "torch.cuda.get_device_capability(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **memory** currently allocated on the device (by tensors in this Python kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DmJmRtTo8BeU",
    "outputId": "bb368a57-803c-47da-e411-5190d7568e92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we only need to understand how to place a tensor on a GPU. It's very simple: most PyTorch routines, which create tensors, **allow to specify which device you'd like to use**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14752\\3022784608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\py4dp\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m                 \"multiprocessing, you must use the 'spawn' start method\")\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "t = torch.tensor([1., 2.], device=\"cuda\")\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use a **specific device** as well (in case you have multiple GPUs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2.], device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([1., 2.], device=device)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the storage for this tensor is different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0\n",
       " 2.0\n",
       "[torch.cuda.FloatStorage of size 2]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU tensors have storage of type `torch.<dtype>Storage`, while GPU tensors have `torch.cuda.<dtype>Storage`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1.0\n",
       " 2.0\n",
       "[torch.FloatStorage of size 2]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([1., 2.]).storage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can repeat the experiment we performed in the previous section without creating any kernel manually (note, that we're using `N` and `M` from the previous section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(N, M, device=device)\n",
    "y = torch.rand(M, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch will perform proper dispatching based on storage type and data type and run an appropriate kernel for us (the actual kernel is buried somewhere under the hood of PyTorch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26.7014, 24.7969, 28.0365,  ..., 29.2888, 27.5010, 27.9509],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of performance, PyTorch is a clear winner due to highly optimized and specialized kernels it has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.4 µs ± 2.92 µs per loop (mean ± std. dev. of 3 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 -r 3 torch.matmul(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU version, as a reminder, is about `100X` times slower (remember, that this number is specific to GPU):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cpu = x.cpu().numpy()\n",
    "y_cpu = y.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.37 ms ± 123 µs per loop (mean ± std. dev. of 3 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit  -n 100 -r 3  np.dot(x_cpu, y_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also, that you first need to take a tensor from the GPU in order to use it in a usual way (with `x.cpu()`).\n",
    "\n",
    "Let's summarize this:\n",
    "\n",
    "- based on tensor location and `dtype`, PyTorch will dispatch a proper call to CPU or GPU implementation of an operation,\n",
    "- hence, the only thing you need to do to start using a GPU is to move all the tensors involved to the GPU.\n",
    "\n",
    "Hence, the only thing you need to do to use the model from HW3 on GPU is to move input tensors and weights to GPU. Nothing else.\n",
    "\n",
    "For multi-GPU training it's a bit more elaborated, but don't worry, PyTorch has a lot of automation and we never create a neural network in the way we did in HW3. You do not need to move each tensor manually: in real coding you'll use `torch.nn.Module` to create your deep learning models (or PyTorch Lightning), which allows to move entire model **at once**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "[Py4DP] [Lecture-3] PyTorch on GPU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
