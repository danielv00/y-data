{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Dask and why we need tools like it\n",
    "\n",
    "There are no problems with processing datasets of up to several Gb, be it some computational task or machine learning model training. Single, although powerful enough, machine can handle such volume easily.\n",
    "\n",
    "It's a bit more elaborated to process tens of `Gb` or more, or **speed-up training** of complex models. Since vertical scaling is always limited by how large the machine is, there's usually no other way, but to go for horizontal scaling and some type of parallelism.\n",
    "\n",
    "**Dask** offers tools for this exact case. For example,\n",
    "\n",
    "- you may want to **leverage all the cores** of your current machine to speed-up the computations, but do not want to to for `multiprocessing`,\n",
    "- alternatively, you may need to **process data too large** for machine's memory, which is called **out-of-core processing**,\n",
    "- or you may need a **unified setup** for both local parallelism (for prototyping) and distributed cloud-based computation.\n",
    "\n",
    "Many interesting problems in machine learning are simply not solvable on a single machine and Dask offers a great and simple way to introduce parallelism into your problem.\n",
    "\n",
    "Another benefit is that Dask is written in Python, so there's no need to use tricky to set up Scala-based Spark.\n",
    "\n",
    "In this tutorial we will use **local** setup, i.e. Dask cluster will run on a single machine. The main benefit is full-utilization of all machine cores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-04T08:02:40.491455Z",
     "start_time": "2020-01-04T08:02:40.487111Z"
    }
   },
   "source": [
    "# Dask cluster\n",
    "\n",
    "First, we need to create a Dask cluster and a Dask client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:02:02.133994Z",
     "start_time": "2020-01-11T15:02:01.069921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from distributed import Client, LocalCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:02:04.777671Z",
     "start_time": "2020-01-11T15:02:03.399063Z"
    }
   },
   "outputs": [],
   "source": [
    "# you may want to change `n_workers` according to your hardware setup\n",
    "cluster = LocalCluster(n_workers=12)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T15:02:06.464505Z",
     "start_time": "2020-01-11T15:02:06.275263Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ad00799fa649b48e1011ff2f89a25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>LocalCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n  <style scoped>\\n    â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a **cluster** of 12 nodes, and connected to it as a client. Note that you can directly control cluster size from the notebook.\n",
    "\n",
    "Under the hood, Dask cluster contains **scheduler**, which is responsible for handling computations and spreading them between nodes. Scheduler can be launched also from the command line (see [Command Line](https://docs.dask.org/en/latest/setup/cli.html) section of documentation).\n",
    "\n",
    "Dask also provides nice realtime **dashboard** to overview tasks and workers (see link in the cell output above).\n",
    "\n",
    "We can now submit tasks to Dask cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:06:44.198861Z",
     "start_time": "2020-01-11T16:06:44.178729Z"
    }
   },
   "outputs": [],
   "source": [
    "result_future = client.submit(np.sin, np.random.randn(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `client.submit` creates what is called **future**, i.e. a handle to task result, which is available as soon as computation completes.\n",
    "\n",
    "You can retrieve task status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:07:47.244817Z",
     "start_time": "2020-01-11T16:07:47.235658Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'finished'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_future.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:08:49.208724Z",
     "start_time": "2020-01-11T16:08:49.186407Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_future.done()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:09:17.606687Z",
     "start_time": "2020-01-11T16:09:17.581838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.38566922,  0.99510528, -0.65454988,  0.34663499, -0.4439334 ,\n",
       "       -0.04220119, -0.06956301,  0.00443238,  0.66714345, -0.65239693,\n",
       "        0.0582789 , -0.96636263,  0.55184192,  0.74382971, -0.06051806,\n",
       "        0.91579664, -0.0572082 ,  0.99856171,  0.03045415, -0.57625148,\n",
       "        0.67631342,  0.5595241 , -0.5897384 ,  0.61344204,  0.99679698,\n",
       "       -0.80810771, -0.84246366,  0.89153179, -0.66727006,  0.3686316 ,\n",
       "        0.93135312, -0.09279354, -0.30981672,  0.18055714, -0.91940546,\n",
       "       -0.84028269, -0.85568167, -0.24937423, -0.06769172,  0.91431261,\n",
       "       -0.95605296,  0.33512032, -0.97361261, -0.1729804 ,  0.99859642,\n",
       "       -0.52965974, -0.36631202,  0.04066818, -0.182421  ,  0.37265006,\n",
       "        0.75720296,  0.69464707, -0.70070384,  0.65185387, -0.92658722,\n",
       "       -0.99572993,  0.62271414,  0.01166503,  0.17914814, -0.97794748,\n",
       "        0.02352513, -0.06039364,  0.99419334,  0.56095852, -0.96081577,\n",
       "        0.81174451, -0.78322288,  0.48709807, -0.1035351 ,  0.37536397,\n",
       "       -0.71151374, -0.96492785, -0.78823715,  0.7942826 , -0.3073254 ,\n",
       "        0.16759195,  0.74718651,  0.86476873, -0.90793947,  0.09550809,\n",
       "        0.85736497, -0.88151096, -0.19745607, -0.97989031, -0.78080938,\n",
       "       -0.10733627, -0.83947051, -0.98430103, -0.31308994, -0.26829757,\n",
       "        0.58133606,  0.15465092, -0.58570971, -0.52867673,  0.17326859,\n",
       "       -0.7313982 , -0.89017304,  0.65597071, -0.88823627,  0.44443001])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:10:11.160658Z",
     "start_time": "2020-01-11T16:10:11.139869Z"
    }
   },
   "source": [
    "You can also submit **multiple tasks** at once (we recommend to open Dask dashboard alongside and observe how tasks start and proceed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:13:46.861073Z",
     "start_time": "2020-01-11T16:13:46.701193Z"
    }
   },
   "outputs": [],
   "source": [
    "futures = [client.submit(np.sin, x) for x in np.random.randn(100)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the results, we need to **gather** them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:14:26.716447Z",
     "start_time": "2020-01-11T16:14:26.636367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.8273538830490595,\n",
       " 0.873022516274151,\n",
       " -0.8861486559438967,\n",
       " 0.675935836098815,\n",
       " 0.9991393832119514,\n",
       " 0.1696600508636658,\n",
       " 0.7003141469465344,\n",
       " -0.8633329987405506,\n",
       " 0.22136867746820688,\n",
       " 0.5089359481444492,\n",
       " -0.5188206326995011,\n",
       " -0.3353401522905225,\n",
       " -0.0330622504074351,\n",
       " 0.24871760442215773,\n",
       " 0.9542510167476598,\n",
       " -0.9401557214029244,\n",
       " -0.5295257407841779,\n",
       " 0.044187215712663326,\n",
       " 0.34734678165519284,\n",
       " 0.9960680537581893,\n",
       " -0.9788079712704486,\n",
       " -0.5229308037953386,\n",
       " 0.9817890136902524,\n",
       " -0.994852732514727,\n",
       " -0.6374301180327867,\n",
       " 0.2509803642770965,\n",
       " -0.9358539062459782,\n",
       " -0.18231491069040157,\n",
       " 0.007604044400980221,\n",
       " 0.12416349105023594,\n",
       " -0.5271477910257577,\n",
       " -0.580232908038599,\n",
       " 0.5083249213558899,\n",
       " 0.20183940171880577,\n",
       " 0.48621894190693815,\n",
       " -0.9349453935133415,\n",
       " 0.05832975506851019,\n",
       " -0.9314275047247508,\n",
       " 0.734893659108188,\n",
       " -0.17826584892600075,\n",
       " -0.9912707147603849,\n",
       " -0.7000887153382841,\n",
       " -0.6925670585980578,\n",
       " -0.29476686555519643,\n",
       " -0.029796807757556665,\n",
       " -0.37675544557345925,\n",
       " 0.011401153181331267,\n",
       " 0.27216420033615313,\n",
       " 0.679204839328053,\n",
       " -0.9000571664700575,\n",
       " 0.11576131836006409,\n",
       " 0.918292657335691,\n",
       " -0.9326945729419603,\n",
       " 0.8955915645604733,\n",
       " 0.41922616324830914,\n",
       " 0.2744503711983982,\n",
       " 0.8666890400806477,\n",
       " -0.5229342506407428,\n",
       " -0.621106817600033,\n",
       " -0.5181798676879146,\n",
       " 0.49688821242069214,\n",
       " 0.6148668999117883,\n",
       " -0.10770160304940345,\n",
       " 0.845486199202782,\n",
       " -0.5833139640345387,\n",
       " 0.7419118394117552,\n",
       " 0.12503938262838296,\n",
       " 0.9456980988678624,\n",
       " -0.7693810299357697,\n",
       " 0.925496133753376,\n",
       " -0.8183275445563013,\n",
       " -0.6276051236462363,\n",
       " 0.8226210525859665,\n",
       " 0.016400239200408534,\n",
       " 0.5508882415181431,\n",
       " 0.9991091821676761,\n",
       " 0.8794418783926086,\n",
       " -0.8698629801742156,\n",
       " -0.6255809325495773,\n",
       " -0.32041023903633636,\n",
       " 0.668109325218588,\n",
       " 0.3967082894863433,\n",
       " -0.7441104947837907,\n",
       " -0.42797417888162614,\n",
       " -0.39900992754036857,\n",
       " -0.23434929467244547,\n",
       " 0.3354140436724864,\n",
       " 0.9718046107610243,\n",
       " -0.1021490477266199,\n",
       " 0.40851892295660924,\n",
       " 0.6483440681263923,\n",
       " -0.7533574487660206,\n",
       " 0.7464144588205381,\n",
       " -0.5102938491684668,\n",
       " 0.8419981140471531,\n",
       " 0.4213002967734003,\n",
       " 0.004125215714864732,\n",
       " -0.9300974714472706,\n",
       " 0.43007408392978375,\n",
       " 0.1037789042254317]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = client.gather(futures)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:14:56.658037Z",
     "start_time": "2020-01-11T16:14:56.645565Z"
    }
   },
   "source": [
    "Dask also allows for straightforward **chaining** of tasks (note that `s`, `s_sq` and `s_full` are all futures, not Numpy arrays):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:26:28.250361Z",
     "start_time": "2020-01-11T16:26:28.206786Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.randn(10000)\n",
    "s = client.submit(np.sin, x)\n",
    "s_sq = client.submit(np.square, x)\n",
    "s_full = client.submit(np.add, s, s_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-11T16:26:46.158275Z",
     "start_time": "2020-01-11T16:26:46.137996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.21614459, -0.11206439, -0.15441179, ...,  0.58596929,\n",
       "        0.55683868,  2.10434109])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_full.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dask and ML\n",
    "\n",
    "As a specific and relevant example of parallelization for machine learning, we will consider parallel grid search. Imagine, that you need to fit a parametrized machine learning model (almost all ML models have some parameters).\n",
    "\n",
    "To find a good set of hyperparameters, you need to fit a model set of parameters. The main Python package for classical machine learning - `scikit-learn` or `sklearn` for short - allows you to do that easily. We will use the Titanic dataset and create a simple classification model for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you may need to change the location according to your local setup\n",
    "DATA_DIR = pathlib.Path(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR.joinpath(\"train.csv\"), index_col=\"PassengerId\")\n",
    "test = pd.read_csv(DATA_DIR.joinpath(\"test.csv\"), index_col=\"PassengerId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess the dataset first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 1 to 891\n",
      "Data columns (total 11 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   Survived  891 non-null    int64  \n",
      " 1   Pclass    891 non-null    int64  \n",
      " 2   Name      891 non-null    object \n",
      " 3   Sex       891 non-null    object \n",
      " 4   Age       714 non-null    float64\n",
      " 5   SibSp     891 non-null    int64  \n",
      " 6   Parch     891 non-null    int64  \n",
      " 7   Ticket    891 non-null    object \n",
      " 8   Fare      891 non-null    float64\n",
      " 9   Cabin     204 non-null    object \n",
      " 10  Embarked  889 non-null    object \n",
      "dtypes: float64(2), int64(4), object(5)\n",
      "memory usage: 83.5+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_imputation = train.groupby([\"Pclass\", \"Sex\"])[\"Age\"].mean()\n",
    "\n",
    "train = train.join(age_imputation,\n",
    "                   on=(\"Pclass\", \"Sex\"),\n",
    "                   rsuffix=\"_imp\")\n",
    "\n",
    "train.loc[train.Age.isnull(), \"Age\"] = train.loc[train.Age.isnull(), \"Age_imp\"]\n",
    "train.drop(\"Age_imp\", axis=1, inplace=True)\n",
    "\n",
    "test = test.join(age_imputation,\n",
    "                 on=(\"Pclass\", \"Sex\"),\n",
    "                 rsuffix=\"_imp\")\n",
    "\n",
    "test.loc[test.Age.isnull(), \"Age\"] = test.loc[test.Age.isnull(), \"Age_imp\"]\n",
    "test.drop(\"Age_imp\", axis=1, inplace=True)\n",
    "\n",
    "most_frequent_port = train.Embarked.value_counts().idxmax()\n",
    "average_fare = train.Fare.mean()\n",
    "\n",
    "train.fillna({\"Embarked\": most_frequent_port}, inplace=True)\n",
    "test.fillna({\"Embarked\": most_frequent_port, \"Fare\": average_fare}, inplace=True)\n",
    "\n",
    "train.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)\n",
    "test.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train, columns=[\"Pclass\", \"Sex\", \"Embarked\"])\n",
    "test = pd.get_dummies(test, columns=[\"Pclass\", \"Sex\", \"Embarked\"])\n",
    "\n",
    "FEATURES_COLS = train.columns[1:]\n",
    "TARGET = \"Survived\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 891 entries, 1 to 891\n",
      "Data columns (total 13 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Survived    891 non-null    int64  \n",
      " 1   Age         891 non-null    float64\n",
      " 2   SibSp       891 non-null    int64  \n",
      " 3   Parch       891 non-null    int64  \n",
      " 4   Fare        891 non-null    float64\n",
      " 5   Pclass_1    891 non-null    uint8  \n",
      " 6   Pclass_2    891 non-null    uint8  \n",
      " 7   Pclass_3    891 non-null    uint8  \n",
      " 8   Sex_female  891 non-null    uint8  \n",
      " 9   Sex_male    891 non-null    uint8  \n",
      " 10  Embarked_C  891 non-null    uint8  \n",
      " 11  Embarked_Q  891 non-null    uint8  \n",
      " 12  Embarked_S  891 non-null    uint8  \n",
      "dtypes: float64(2), int64(3), uint8(8)\n",
      "memory usage: 48.7 KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 418 entries, 892 to 1309\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   Age         418 non-null    float64\n",
      " 1   SibSp       418 non-null    int64  \n",
      " 2   Parch       418 non-null    int64  \n",
      " 3   Fare        418 non-null    float64\n",
      " 4   Pclass_1    418 non-null    uint8  \n",
      " 5   Pclass_2    418 non-null    uint8  \n",
      " 6   Pclass_3    418 non-null    uint8  \n",
      " 7   Sex_female  418 non-null    uint8  \n",
      " 8   Sex_male    418 non-null    uint8  \n",
      " 9   Embarked_C  418 non-null    uint8  \n",
      " 10  Embarked_Q  418 non-null    uint8  \n",
      " 11  Embarked_S  418 non-null    uint8  \n",
      "dtypes: float64(2), int64(2), uint8(8)\n",
      "memory usage: 19.6 KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now our dataset contains no missing values, is fully numeric, so that we can start modeling it. We will use random forest model. You do not need to understand it in full right now, but the main idea is to combine a lot of weak estimators (decision trees) and get a better result overall.\n",
    "\n",
    "We will also use cross-validation, since it's a crucial part of hyperparameters search. The main idea is, again, simple: you train your models on a part of a dataset, you choose model parameters based on model performance on a different part (previously unseen to reduce overfitting risk, i.e. you cross-validate your model), and then you assess the final model performance with the best parameters on a test set. i.e. test your final model.\n",
    "\n",
    "`sklearn` provides convenient classes for the entire grid search process. We will use 4-fold cross-validation: for each set of parameters, training dataset will be split in 4 equal parts, and four models will be fitted with that set of parameters in such a way that each model is cross-validated on one of four fold, and the remaining 3 are used for training.\n",
    "\n",
    "`joblib` is the job manager, which dispatches calculations under the hood and can use different backends to do that in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to specify parameters grid. During grid search, all combinations will be used for fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"max_depth\": [2,4,6],\n",
    "    \"n_estimators\": [100, 200, 500],\n",
    "    \"class_weight\": [None, \"balanced\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a model instance, so that `sklearn` knows which model we want. In `sklearn` API model parameters can be directly set, hence, we create an \"empty\" model, which will serve as a blueprint.\n",
    "\n",
    "Also note, that we do not provide the scoring criterion. By default, `GridSearchCV` will use whatever scoring the model uses. In the case of `RandomForestClassifier` it's accuracy, which is exactly the metrics Kaggle uses for this dataset.\n",
    "\n",
    "We now can launch the grid search itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 18 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend DaskDistributedBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.6s\n",
      "[Parallel(n_jobs=-1)]: Done  72 out of  72 | elapsed:   24.2s finished\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier()\n",
    "grid_cv = GridSearchCV(model, params, cv=4, verbose=1)\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    grid_cv.fit(train[FEATURES_COLS], train[TARGET])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`joblib` will use the local cluster we created to distribute the training jobs and run them in parallel. The best parameters for the features we have are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_weight': None, 'max_depth': 6, 'n_estimators': 100}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best score, correspondingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8237991354583283"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_cv.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When grid search finds the best parameters, by default it refits the model on the entire training set, so that we do not need to do that manually. Effectively, we now have a random forest model, trained on the entire training set with the best model parameters. Let's use it for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(grid_cv.best_estimator_.predict(test[FEATURES_COLS]),\n",
    "                          index=test.index, columns=[\"Survived\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived\n",
       "PassengerId          \n",
       "892                 0\n",
       "893                 0\n",
       "894                 0\n",
       "895                 0\n",
       "896                 1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(DATA_DIR.joinpath(\"dask_submission.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These predictions get about `0.775` when submitted to Kaggle.\n",
    "\n",
    "\n",
    "# Final remarks\n",
    "\n",
    "You may consider this an overkill for this specific model. That is true, and anyway `joblib` can handle local parallelism well enough. However, imagine that you're searching over a **huge grid** and have a **standby Dask cluster in the cloud**: in that case this setup will serve its purpose really well.\n",
    "\n",
    "We haven't covered a lot of technical details about Dask (resource quoting, deployment and others), as well as out-of-core processing, but hopefully you got a feeling of it and will dig further as soon as you'll encounter long-running grid/random search or alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
