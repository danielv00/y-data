{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/inbarhub/YDATA_DL_assignments_2021-2022/blob/main/H.W_9_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88X5OhoAJEJh"
   },
   "source": [
    "# RNN for text generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yh_aPOY6JEJz"
   },
   "source": [
    "In this exercise, you'll unleash the hidden creativity of your computer, by letting it generate Country songs (yeehaw!). You'll train a character-level RNN-based language model, and use it to generate new songs.\n",
    "\n",
    "\n",
    "### Special Note\n",
    "\n",
    "Our Deep Learning course was packed with both theory and practice. In a short time, you've got to learn the basics of deep learning theory and get hands-on experience training and using pretrained DL networks, while learning PyTorch.  \n",
    "Past exercises required a lot of work, and hopefully gave you a sense of the challenges and difficulties one faces when using deep learning in the real world. While the investment you've made in the course so far is enormous, We strongly encourage you to take a stab at this exercise. \n",
    "\n",
    "Some songs contain no lyrics (for example, they just contain the text \"instrumental\"). Others include non-English characters. You'll often need to preprocess your data and make decisions as to what your network should actually get as input (think - how should you treat newline characters?)\n",
    "\n",
    "More issues will probably pop up while you're working on this task. If you face technical difficulties or find a step in the process that takes too long, please let me know. It would also be great if you share with the class code you wrote that speeds up some of the work (for example, a data loader class, a parsed dataset etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npjwCeQRJEJ_"
   },
   "source": [
    "## RNN for Text Generation\n",
    "In this section, we'll use an LSTM to generate new songs. You can pick any genre you like, or just use all genres. You can even try to generate songs in the style of a certain artist - remember that the Metrolyrics dataset contains the author of each song. \n",
    "\n",
    "For this, we’ll first train a character-based language model. We’ve mostly discussed in class the usage of RNNs to predict the next word given past words, but as we’ve mentioned in class, RNNs can also be used to learn sequences of characters.\n",
    "\n",
    "First, please go through the [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) on generating family names. You can download a .py file or a jupyter notebook with the entire code of the tutorial. \n",
    "\n",
    "As a reminder of topics we've discussed in class, see Andrej Karpathy's popular blog post [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). You are also encouraged to view [this](https://gist.github.com/karpathy/d4dee566867f8291f086) vanilla implementation of a character-level RNN, written in numpy with just 100 lines of code, including the forward and backward passes.  \n",
    "\n",
    "Other tutorials that might prove useful:\n",
    "1. http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/\n",
    "1. https://github.com/mcleonard/pytorch-charRNN\n",
    "1. https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0RjfaAf2N9dq"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Functional\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn import functional as F\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import nltk\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VGu71jqCJEKE",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>sent</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fully-dressed</td>\n",
       "      <td>2008</td>\n",
       "      <td>annie</td>\n",
       "      <td>Pop</td>\n",
       "      <td>[HEALY]\\n[spoken] This is Bert Healy saying .....</td>\n",
       "      <td>1041</td>\n",
       "      <td>healy spoken this bert healy saying singing he...</td>\n",
       "      <td>826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>surrounded-by-hoes</td>\n",
       "      <td>2006</td>\n",
       "      <td>50-cent</td>\n",
       "      <td>Hip-Hop</td>\n",
       "      <td>[Chorus: repeat 2X] Even when I'm tryin to be ...</td>\n",
       "      <td>1392</td>\n",
       "      <td>chorus repeat x even i tryin low i recognized ...</td>\n",
       "      <td>884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>taste-the-tears-thunderpuss-remix</td>\n",
       "      <td>2006</td>\n",
       "      <td>amber</td>\n",
       "      <td>Pop</td>\n",
       "      <td>How could you cause me so much pain?\\nAnd leav...</td>\n",
       "      <td>1113</td>\n",
       "      <td>how could cause much pain and leave heart rain...</td>\n",
       "      <td>756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the-truth-will-set-me-free</td>\n",
       "      <td>2006</td>\n",
       "      <td>glenn-hughes</td>\n",
       "      <td>Rock</td>\n",
       "      <td>In a scarlet vision\\nIn a velvet room\\nI come ...</td>\n",
       "      <td>779</td>\n",
       "      <td>in scarlet vision in velvet room i come decisi...</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the-last-goodbye</td>\n",
       "      <td>2008</td>\n",
       "      <td>aaron-pritchett</td>\n",
       "      <td>Country</td>\n",
       "      <td>Sprintime in Savannah\\nIt dont get much pretti...</td>\n",
       "      <td>881</td>\n",
       "      <td>sprintime savannah it dont get much prettier b...</td>\n",
       "      <td>639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                song  year           artist    genre  \\\n",
       "0                      fully-dressed  2008            annie      Pop   \n",
       "1                 surrounded-by-hoes  2006          50-cent  Hip-Hop   \n",
       "2  taste-the-tears-thunderpuss-remix  2006            amber      Pop   \n",
       "3         the-truth-will-set-me-free  2006     glenn-hughes     Rock   \n",
       "4                   the-last-goodbye  2008  aaron-pritchett  Country   \n",
       "\n",
       "                                              lyrics  num_chars  \\\n",
       "0  [HEALY]\\n[spoken] This is Bert Healy saying .....       1041   \n",
       "1  [Chorus: repeat 2X] Even when I'm tryin to be ...       1392   \n",
       "2  How could you cause me so much pain?\\nAnd leav...       1113   \n",
       "3  In a scarlet vision\\nIn a velvet room\\nI come ...        779   \n",
       "4  Sprintime in Savannah\\nIt dont get much pretti...        881   \n",
       "\n",
       "                                                sent  num_words  \n",
       "0  healy spoken this bert healy saying singing he...        826  \n",
       "1  chorus repeat x even i tryin low i recognized ...        884  \n",
       "2  how could cause much pain and leave heart rain...        756  \n",
       "3  in scarlet vision in velvet room i come decisi...        583  \n",
       "4  sprintime savannah it dont get much prettier b...        639  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lyrics = pd.read_parquet('metrolyrics.parquet')\n",
    "df_lyrics = df_lyrics.reset_index(drop=True)\n",
    "df_lyrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(lyric):\n",
    "    return re.sub(\"[^a-z' ]\", \"\", lyric).replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On a hill, inside a house in Covewell Reach',\n",
       " \"Stands a man who's feeling very tired\",\n",
       " 'Looking at a song he wrote some time ago',\n",
       " 'Could have made it big inside a Broadway show',\n",
       " 'Every day I go away and find ideas',\n",
       " \"Think I'll climb on top of somewhere high\",\n",
       " \"Couldn't I write a song about a man who's dead?\",\n",
       " \"Didn't really know if he was off his head\",\n",
       " \"Ev'rybody knows, that's the way it goes\",\n",
       " 'Too bad for Gilbert Green']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\\n\".join(df_lyrics[df_lyrics.artist==\"bee-gees\"].lyrics.tolist())\n",
    "text = text.replace(\"\\n\\n\", \"\\n\")\n",
    "text.splitlines()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = text.lower().split(\"\\n\")\n",
    "lyrics = np.unique(lyrics)[1:].tolist()\n",
    "\n",
    "clean_lyrics = [process_text(lyric) for lyric in lyrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([' each night before we go to sleep',\n",
       "  'i love you',\n",
       "  'cause i believed in you',\n",
       "  'cause i dont wanna feel the pain anymore',\n",
       "  'cause i know it isnt heaven is it love or hate'],\n",
       " [' each night before we go to sleep',\n",
       "  '\"i love you\"',\n",
       "  \"'cause i believed in you\",\n",
       "  \"'cause i don't wanna feel the pain anymore\",\n",
       "  \"'cause i know it isn't heaven is it love or hate\"])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_lyrics[:5], lyrics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_grams(words, n_gram_size):\n",
    "    n_grams = []\n",
    "    \n",
    "    if (n_gram_size <= 0):\n",
    "        raise Exception(\"n_gram_size should be higher than zero!\")\n",
    "        \n",
    "    n_gram_size = n_gram_size - 1\n",
    "\n",
    "    if len(words.split()) <= n_gram_size:\n",
    "        return [words]\n",
    "    \n",
    "    for itr in range(n_gram_size, len(words.split())):\n",
    "        curr_seq = words.split()[itr - n_gram_size:itr + 1]\n",
    "        n_grams.append(\" \".join(curr_seq))\n",
    "    \n",
    "    return n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['each night', 'night before', 'before we', 'we go', 'go to', 'to sleep']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n_grams(clean_lyrics[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_size = 4\n",
    "n_grams = [generate_n_grams(lyric, n_gram_size) for lyric in clean_lyrics]\n",
    "phrases = np.unique(np.array(sum(n_grams, []))).tolist()\n",
    "\n",
    "distinct_words = np.unique(np.array(\" \".join(phrases).split(\" \")))\n",
    "distinct_words_idx = np.arange(distinct_words.size)\n",
    "word_to_idx = dict(zip(distinct_words.tolist(), distinct_words_idx.tolist()))\n",
    "idx_to_word = dict(zip(distinct_words_idx.tolist(), distinct_words.tolist()))\n",
    "vocabulary_size = len(word_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2136,\n",
       " ['',\n",
       "  'a afallin for you',\n",
       "  'a bad girl your',\n",
       "  'a beat of a',\n",
       "  'a bed of leaves',\n",
       "  'a bit close to',\n",
       "  'a body to behold',\n",
       "  'a body you dream',\n",
       "  'a boy all the',\n",
       "  'a brave new world'],\n",
       " ['youve got nothing to',\n",
       "  'youve got the best',\n",
       "  'youve got the first',\n",
       "  'youve got to be',\n",
       "  'youve got to find',\n",
       "  'youve got to live',\n",
       "  'youve got to wear',\n",
       "  'youve nothing to hide',\n",
       "  'youve shown it inside',\n",
       "  'youve stayed with other'],\n",
       " [('', 0),\n",
       "  ('a', 1),\n",
       "  ('aah', 2),\n",
       "  ('able', 3),\n",
       "  ('aboard', 4),\n",
       "  ('about', 5),\n",
       "  ('above', 6),\n",
       "  ('accused', 7),\n",
       "  ('ace', 8),\n",
       "  ('aches', 9)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size, phrases[:10],phrases[-10:], list(word_to_idx.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_word = []\n",
    "y_word = []\n",
    "\n",
    "for phrase in phrases:\n",
    "    if (len(phrase.split()) != n_gram_size):\n",
    "        continue\n",
    "    \n",
    "    x_word.append(\" \".join(phrase.split()[:-1]))\n",
    "    y_word.append(\" \".join(phrase.split()[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['a afallin for', 'a bad girl', 'a beat of'],\n",
       " ['afallin for you', 'bad girl your', 'beat of a'])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_word[:3], y_word[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phrase_idx(phrase):\n",
    "    return [word_to_idx[word] for word in phrase.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_idx = np.array([get_phrase_idx(word) for word in x_word])\n",
    "y_idx = np.array([get_phrase_idx(word) for word in y_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[   1,   20,  670],\n",
       "        [   1,  103,  721],\n",
       "        [   1,  117, 1231],\n",
       "        ...,\n",
       "        [2134, 1222, 1874],\n",
       "        [2134, 1581,  923],\n",
       "        [2134, 1695, 2066]]),\n",
       " array([[  20,  670, 2121],\n",
       "        [ 103,  721, 2129],\n",
       "        [ 117, 1231,    1],\n",
       "        ...,\n",
       "        [1222, 1874,  836],\n",
       "        [1581,  923,  907],\n",
       "        [1695, 2066, 1261]]),\n",
       " (11935, 3),\n",
       " (11935, 3))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_idx, y_idx, y_idx.shape, x_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_layers, num_layers, embedding_size, drop_prob, lr):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lr = lr\n",
    "        self.embedded = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_layers, num_layers, dropout = drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_layers, vocabulary_size)      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embedded(x)     \n",
    "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
    "        dropout_out = self.dropout(lstm_output).reshape(-1, self.hidden_layers) \n",
    "        out = self.fc(dropout_out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_layers).zero_(),\n",
    "                  weight.new(self.num_layers, batch_size, self.hidden_layers).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "hidden_layers = 256\n",
    "num_layers = 4\n",
    "embedding_size = 200\n",
    "drop_prob = 0.3\n",
    "lr = 0.001\n",
    "batch_size = 32\n",
    "\n",
    "model = LSTM(hidden_layers, num_layers, embedding_size, drop_prob, lr).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedded): Embedding(2136, 200)\n",
       "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=2136, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(x, y, batch_size):\n",
    "    for itr in range(batch_size, x.shape[0], batch_size):\n",
    "        batch_x = x[itr - batch_size:itr, :]\n",
    "        batch_y = y[itr - batch_size:itr, :]\n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        hidden_layer = model.init_hidden(batch_size)\n",
    "        for x, y in next_batch(x_idx, y_idx, batch_size):\n",
    "            inputs = torch.from_numpy(x).type(torch.LongTensor).to(device)\n",
    "            act = torch.from_numpy(y).type(torch.LongTensor).to(device)\n",
    "            hidden_layer = tuple([layer.data for layer in hidden_layer])\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(inputs, hidden_layer)\n",
    "            loss = loss_func(output, act.view(-1))\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "            optimizer.step()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9244218f9619491095582ddaeb10bdc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(num_epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, dim=None, temperature = 1.):\n",
    "    e_x = torch.exp(x / temperature)\n",
    "    return e_x / torch.sum(e_x, dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, token, hidden_layer):\n",
    "    x = np.array([[word_to_idx[token] if token in word_to_idx else len(word_to_idx)]])\n",
    "    inputs = torch.from_numpy(x).type(torch.LongTensor).to(device)\n",
    "    hidden = tuple([layer.data for layer in hidden_layer])\n",
    "    out, hidden = model(inputs, hidden)\n",
    "    prob = softmax(out, dim=1, temperature=np.random.choice([0.5,0.2,0.1], p=[0.5, 0.25, 0.25]))\n",
    "    prob = prob.detach().cpu().numpy()\n",
    "    prob = prob.reshape(prob.shape[1],)\n",
    "    top_tokens = prob.argsort()[-3:][::-1]\n",
    "    selected_index = top_tokens[0]\n",
    "\n",
    "    return idx_to_word[selected_index], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lyrics(model, limit_words, start_text):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "    tokens = start_text.split()\n",
    "    \n",
    "    for token in start_text.split():\n",
    "        curr_token, hidden = predict(model, token, hidden)\n",
    "    \n",
    "    tokens.append(curr_token)\n",
    "    \n",
    "    for token_num in range(limit_words - 1):\n",
    "        token, hidden = predict(model, tokens[-1], hidden)\n",
    "        tokens.append(token)\n",
    "        \n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song</th>\n",
       "      <th>year</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>lyrics</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>sent</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>gilbert-green</td>\n",
       "      <td>2006</td>\n",
       "      <td>bee-gees</td>\n",
       "      <td>Pop</td>\n",
       "      <td>On a hill, inside a house in Covewell Reach\\nS...</td>\n",
       "      <td>1000</td>\n",
       "      <td>on hill inside house covewell reach stands man...</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>if-i-can-t-have-you-remix</td>\n",
       "      <td>2007</td>\n",
       "      <td>bee-gees</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Don't know why\\nI'm surviving every lonely day...</td>\n",
       "      <td>1198</td>\n",
       "      <td>don know i surviving every lonely day when got...</td>\n",
       "      <td>674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>cover-you</td>\n",
       "      <td>2006</td>\n",
       "      <td>bee-gees</td>\n",
       "      <td>Pop</td>\n",
       "      <td>You could read her lips well as I was able\\nSh...</td>\n",
       "      <td>1086</td>\n",
       "      <td>you could read lips well i able she taking i l...</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>night-fever-remix</td>\n",
       "      <td>2007</td>\n",
       "      <td>bee-gees</td>\n",
       "      <td>Pop</td>\n",
       "      <td>Listen to the ground\\nThere is movement all ar...</td>\n",
       "      <td>1503</td>\n",
       "      <td>listen ground there movement around there some...</td>\n",
       "      <td>982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>the-love-of-a-woman</td>\n",
       "      <td>2006</td>\n",
       "      <td>bee-gees</td>\n",
       "      <td>Pop</td>\n",
       "      <td>When the day is done , and the night is near\\n...</td>\n",
       "      <td>565</td>\n",
       "      <td>when day done night near happiness gone and ga...</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           song  year    artist genre  \\\n",
       "109               gilbert-green  2006  bee-gees   Pop   \n",
       "142   if-i-can-t-have-you-remix  2007  bee-gees   Pop   \n",
       "486                   cover-you  2006  bee-gees   Pop   \n",
       "739           night-fever-remix  2007  bee-gees   Pop   \n",
       "1154        the-love-of-a-woman  2006  bee-gees   Pop   \n",
       "\n",
       "                                                 lyrics  num_chars  \\\n",
       "109   On a hill, inside a house in Covewell Reach\\nS...       1000   \n",
       "142   Don't know why\\nI'm surviving every lonely day...       1198   \n",
       "486   You could read her lips well as I was able\\nSh...       1086   \n",
       "739   Listen to the ground\\nThere is movement all ar...       1503   \n",
       "1154  When the day is done , and the night is near\\n...        565   \n",
       "\n",
       "                                                   sent  num_words  \n",
       "109   on hill inside house covewell reach stands man...        671  \n",
       "142   don know i surviving every lonely day when got...        674  \n",
       "486   you could read lips well i able she taking i l...        671  \n",
       "739   listen ground there movement around there some...        982  \n",
       "1154  when day done night near happiness gone and ga...        365  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df_lyrics[df_lyrics.artist==\"bee-gees\"]\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "tokens = [w for s in corpus.sent.tolist() for w in WordPunctTokenizer().tokenize(s)]\n",
    "word_fd = nltk.FreqDist(tokens)\n",
    "bigram_fd = nltk.FreqDist(nltk.bigrams(tokens))\n",
    "finder = BigramCollocationFinder(word_fd, bigram_fd)\n",
    "collocations = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "collocations = pd.DataFrame(collocations)\n",
    "collocations.columns = [\"col\", \"score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45783    Is this your voice I heard\\nSpeakin' my name\\n...\n",
       "Name: lyrics, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.sample(1).lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lyric(start_text, limit_lines):\n",
    "    count_words = list(map(lambda s:s.count(\" \"), next(iter(corpus.sample(1).lyrics)).split(\"\\n\")))\n",
    "    lines = [generate_lyrics(model, count_words[0], start_text.lower())]\n",
    "\n",
    "    for i in range(limit_lines - 1):\n",
    "        start_text = \" \".join(next(iter(collocations.sample(1)[\"col\"])))\n",
    "        lines.append(generate_lyrics(model, count_words[i%len(count_words)], start_text.lower()))\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this way i know i know i know i know i\n",
      "born one girl and fade as you know that i know\n",
      "break their love and make me\n",
      "face angel on the love is true and fade\n",
      "breath body love and make me cry and the love\n"
     ]
    }
   ],
   "source": [
    "print(get_lyric(\"this way\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this way i know i know\n",
      "crime and i know i know\n",
      "am i know i know i know\n",
      "ends i know i know i\n",
      "wise it not to be a love in the love\n"
     ]
    }
   ],
   "source": [
    "print(get_lyric(\"this way\", 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nebraska i know\n",
      "soul you know i\n",
      "sever love and be a love and\n",
      "town you very alone and i know\n",
      "it burning to be a love in\n"
     ]
    }
   ],
   "source": [
    "print(get_lyric(\"nebraska\", 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUeT3OS9JEKF"
   },
   "source": [
    "### Final Tips\n",
    "As a final tip, we do encourage you to do most of the work first on your local machine. They say that Data Scientists spend 80% of their time cleaning the data and preparing it for training (and 20% complaining about cleaning the data and preparing it). Handling these parts on your local machine usually mean you will spend less time complaining. You can switch to the cloud once your code runs and your pipeline is in place, for the actual training using a GPU.  \n",
    "\n",
    "We also encourage you to use a small subset of the dataset first, so things run smoothly. The Metrolyrics dataset contains over 300k songs. You can start with a much much smaller set (even 3,000 songs) and try to train a network based on it. Once everything runs properly, add more data. \n",
    "\n",
    "Good luck!  \n",
    "\n",
    "---\n",
    "#### This exericse was originally written by Dr. Omri Allouche."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "H.W_9_Text_Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
