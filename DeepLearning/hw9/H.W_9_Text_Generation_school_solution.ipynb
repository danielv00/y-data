{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1ZVWOYeLI_i"
   },
   "source": [
    "# RNN for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2X_pYJYLI_s"
   },
   "source": [
    "In this exercise, you'll unleash the hidden creativity of your computer, by letting it generate Country songs (yeehaw!). You'll train a character-level RNN-based language model, and use it to generate new songs.\n",
    "\n",
    "\n",
    "### Special Note\n",
    "\n",
    "Our Deep Learning course was packed with both theory and practice. In a short time, you've got to learn the basics of deep learning theory and get hands-on experience training and using pretrained DL networks, while learning PyTorch.\n",
    "Past exercises required a lot of work, and hopefully gave you a sense of the challenges and difficulties one faces when using deep learning in the real world. While the investment you've made in the course so far is enormous, We strongly encourage you to take a stab at this exercise.\n",
    "\n",
    "Some songs contain no lyrics (for example, they just contain the text \"instrumental\"). Others include non-English characters. You'll often need to preprocess your data and make decisions as to what your network should actually get as input (think - how should you treat newline characters?)\n",
    "\n",
    "More issues will probably pop up while you're working on this task. If you face technical difficulties or find a step in the process that takes too long, please let me know. It would also be great if you share with the class code you wrote that speeds up some of the work (for example, a data loader class, a parsed dataset etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovaAWy6kLI_w"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8hsrz8u4LI_x"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "import spacy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import string\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45lYrWpILI_y"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qbfqc45ULI_z"
   },
   "source": [
    "#### Modify spaCy tokenization rules so that it allows contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IpaAYXeLI_z"
   },
   "outputs": [],
   "source": [
    "nlp.tokenizer.rules = {key: value for key, value in nlp.tokenizer.rules.items() if \"'\" not in key and \"’\" not in key and \"‘\" not in key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWScmcSULI_0"
   },
   "outputs": [],
   "source": [
    "lyrics_df = pd.read_parquet(\"https://raw.githubusercontent.com/omriallouche/ydata_deep_learning_2021/master/data/metrolyrics.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFCwUZjPLI_1"
   },
   "source": [
    "### Pre-processing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvA-ER_MLI_4"
   },
   "source": [
    "<b>ISSUE: Some songs contain no lyrics (for example, they just contain the text \"instrumental\"). Others include non-English characters. You'll often need to preprocess your data and make decisions as to what your network should actually get as input (think - how should you treat newline characters?) <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1BgTSC1LI_5"
   },
   "source": [
    "<b> *Comments:* <b> \n",
    "\n",
    "    (Output is displayed below)\n",
    "- Lyrics that just contain the word \"instrumental\" are still informative. This will likely point to the type of genre that it is.\n",
    "- Non-English characters are relevant as input. Even if the lyrics are not readable, it is possible that certain language patterns will appear in specific genres.\n",
    "- Some songs say \"We are not in a position to display these lyrics due to licensing restrictions. Sorry for the inconvinience.\". These should be removed from the dataset, since there is no useful information in the lyrics.\n",
    "- Some genres may have shorter lines and thus, new line character counts and locations may be relevant to the model. \n",
    "- Some \"lyrics\" are simply descriptions of the song. For example: \"[habituation to the use of cannabis][sung by Ron from Mandrake & arclila acoustic guitar by Tony Perez].\" Unfortunately there are no ways of identifying these songs specifically, so they will be kept in and will probably damage the performance of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5MGmtu-LI_5"
   },
   "source": [
    "#### Analyzing songs with under 20 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rkkVc7pLI_5",
    "outputId": "b131263d-bfa1-451e-9b5b-273a283b2466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genre: Metal\n",
      "Song:\n",
      "Snorted salvation\n",
      "Methamphetamine inhilation\n",
      "An army of skeletal\n",
      "Zombies looking for a fix\n",
      "Survival of the sick\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Faal I flammar\n",
      "Fanga under is\n",
      "Mara rir evig\n",
      "Nightmare\n",
      "Fall in flames\n",
      "Trapped under ice\n",
      "Ridden by nightmares forever\n",
      "\n",
      "Genre: Hip-Hop\n",
      "Song:\n",
      "Hiding emotions, concealing the truth.\n",
      "(Understand, understand)\n",
      "Feelings that should be shared, held inside.\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Anyone who has intelligence,\n",
      "may interpret the number of the beast.\n",
      "It's a man's number.\n",
      "This number is 666.\n",
      "\n",
      "Genre: Pop\n",
      "Song:\n",
      "Lady killerrrrrrrrrrrrrrrrrrrrrrrr...\n",
      "Be warned that the physical might be killer [echoes]\n",
      "Lady killerrrrrrrrrrrrrrrrrrrrrrrr...\n",
      "\n",
      "Genre: Pop\n",
      "Song:\n",
      "Der Schlange Kur\n",
      "Ist nur L'amour, for shure\n",
      "Der Schlange Kur toujurs\n",
      "Anakondamour.\n",
      "BEMERKUNG: der ist wirklich nicht länger...\n",
      "\n",
      "Genre: Country\n",
      "Song:\n",
      "We are not in a position to display these lyrics due to licensing restrictions. Sorry for the inconvinience.\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "Instrumental\n",
      "Instrumental\n",
      "Instrumental\n",
      "Instrumental\n",
      "Instrumental\n",
      "Instrumental\n",
      "Instrumental\n",
      "Instrumental\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "Seni ilk gÃ¶rdÃ¼ÄÃ¼mde\n",
      "YeÅil bir elbiseyle\n",
      "DÃ¼nya dÃ¶ndÃ¼ tersine\n",
      "SÄ±cak bir gÃ¼lÃ¼msemenle\n",
      "HaykÄ±rsam duyar mÄ±sÄ±n beni?\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "(Hallucinating back to the event of birth.\n",
      "Slowly regressing into primitive\n",
      "stages of human evolution: and beyond.)\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "There is a mist that chokes the land.\n",
      "The waves attack. Relent.\n",
      "The skies attack,\n",
      "they come, relentless.\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Unter dem Schein des Mondes\n",
      "auf dem von Fackeln erleuchteten Pfad.\n",
      "Blutige Finger, gespaltener Geist\n",
      "ngstliche Blicke\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Nightmares return again,\n",
      "There's no way to make amends,\n",
      "His heart and the battlefield,\n",
      "Are alike silent and empty.\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Crooked I Ft. Slaughterhouse - Monsters In My Head\n",
      "Crooked I Ft. Slaughterhouse - Monsters In My Head\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      ".negozi bomboniere milano\n",
      "immagini da colorare dragonball\n",
      "mulatas\n",
      "storie di incesti con padre\n",
      "winnie da colorare\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "Lied to threatened, cheated and deceived\n",
      "Hear nothing see nothing say nothing\n",
      "Led up garden paths and into blind alleys\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "We don't need no calculator. We've got the math. We don't need no measurements. And we don't need a graph.\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "INSTRUMENTAL: MUSIC BY CAFFEINE& ZEKI UNGOR\n",
      "LYRICS BY MEHMET AKIF ERSOY\n",
      "YAPILMASI LAZIM OLDUGU ICIN YAPTIK\n",
      "\n",
      "Genre: Pop\n",
      "Song:\n",
      "Bingo, Bingo, B-B-Bingo, Bango.\n",
      "Bingo, Bingo, B-B-Bingo, Bango.\n",
      "Bingo, Bingo, B-B-Bingo, Bango.\n",
      "Bingo, Bingo, B-B-Bingo, Bango.\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "(Latimer/Bardens)\n",
      "Andrew Latimer: Guitar\n",
      "Peter Bardens: Organ\n",
      "Doug Ferguson: Bass\n",
      "Andy Ward: Drums, Congas\n",
      "\n",
      "Genre: Hip-Hop\n",
      "Song:\n",
      "Teaching lessons, boy\n",
      "With the Smith & Wesson, boy\n",
      "Just doin' damage, boy\n",
      "I'mma squeeze this lemon, boy\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "[habituation to the use of cannabis]\n",
      "[sung by Ron from Mandrake & arclila acoustic guitar by Tony Perez]\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "He Moa Kane (ancient Hawaiian riddle)\n",
      "Above is red -- below is red\n",
      "Hear the ringing voice...\n",
      "A strutting rooster\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Mystic sky and mystic earth\n",
      "this is my inner world\n",
      "world without lies and false\n",
      "this is my own world...\n",
      "\n",
      "Genre: Hip-Hop\n",
      "Song:\n",
      "Fat Joe Ft. French Montana Welcome To The Darkside\n",
      "Fat Joe Ft. French Montana Welcome To The Darkside\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "Written by elton john\n",
      "Available on the various artists album disney's for our children\n",
      "[instrumental]\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Stefan Hertrich: Vocals/Guitar\n",
      "Andi Wecker: Guitar\n",
      "Thomas Herrmann: Lead Guitar\n",
      "Rico Galvagno: Bass\n",
      "Harald Winkler: Drums/Vocals\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Ihr Helden, schwelget in Wonnen\n",
      "Shne Odins seid willkommen\n",
      "Labet euch an Met und Weibe\n",
      "Leget ab das Trauerkleide.\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "God speaks, with cold tongue\n",
      "Receive, my open soul\n",
      "Control\n",
      "Shattered soul, look past\n",
      "Watch them follow, watch them die\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "The ultimate serach of mankind\n",
      "We experience the blessed infinity\n",
      "'Cause only among mortals\n",
      "You'll find immortality\n",
      "\n",
      "Genre: Hip-Hop\n",
      "Song:\n",
      "These niggas pussies\n",
      "They ain't out here, they ain't ridin'\n",
      "My niggas with me\n",
      "Yes, they ridin'; ya'll hidin'\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Manic Depression!\n",
      "My spastic obsession!\n",
      "Lose and fall and fail and stop.\n",
      "I make my way back from the top\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "We are not in a position to display these lyrics due to licensing restrictions. Sorry for the inconvinience.\n",
      "\n",
      "Genre: Pop\n",
      "Song:\n",
      "Keep Breathing\n",
      "Keep Keep Breathing in love\n",
      "just keep breathing keepkeep breathin in love (love\n",
      "my heart is broken\n",
      "\n",
      "Genre: Country\n",
      "Song:\n",
      "We are not in a position to display these lyrics due to licensing restrictions. Sorry for the inconvinience.\n",
      "\n",
      "Genre: Pop\n",
      "Song:\n",
      "Radio Chatter by Francisco Lyrics\n",
      "Radio Chatter by Francisco Lyrics\n",
      "Radio Chatter by Francisco Lyrics\n",
      "\n",
      "Genre: Hip-Hop\n",
      "Song:\n",
      "We are not in a position to display these lyrics due to licensing restrictions. Sorry for the inconvinience.\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "A young child when sodomized\n",
      "Will shut down all systems\n",
      "A young infant who is sodomized\n",
      "Immediately goes to sleep\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Dubhe uber karibischem Eiland\n",
      "Hinwandelnd zwischen saulen\n",
      "Und doch\n",
      "Kein laut Lokrischer Traum\n",
      "[Music: Kailer / Subtext: Waldura]\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "shaking hands.\n",
      "greedy smile.\n",
      "on the window sill.\n",
      "flowered walks.\n",
      "wooded field.\n",
      "on the window sill.\n",
      "sirens come\n",
      "\n",
      "Genre: Hip-Hop\n",
      "Song:\n",
      "And now\n",
      "The sound of photography from\n",
      "Dilated\n",
      "Ladies and gentlemen, without no doubt\n",
      "The sound of photography from\n",
      "Dilated\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Thrill, kill the black cult\n",
      "Walk among the weak by night\n",
      "Affliction...\n",
      "Deep as any burial\n",
      "Take their worthless lives...\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "We starve our children before the banquet of knowledge while politicians indulge in the feasts of war.\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Es ist zu hei, zeigt mein Negativ.\n",
      "Zeigt das Negativ.\n",
      "Das Negativ.\n",
      "Das Negativ.\n",
      "Negativ,\n",
      "Gativ. Vitagen!\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "I've said everything I could feel\n",
      "Think everything I could think\n",
      "Heard every word without listening\n",
      "I'm a dick\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Impulses to self-destruction\n",
      "the need for better body armor\n",
      "is ever increasing.\n",
      "Shooting blindly into the crowd.\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "You're a prisoner of your ideals\n",
      "Just a prisoner of your materialistic ideals\n",
      "You're a prisoner of your worthless ideals\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "Censorship on the left and right\n",
      "watch what you think\n",
      "watch what you say\n",
      "Enforced Orthodoxy\n",
      "Enforced Orthodoxy\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "Walking a ho stroll\n",
      "Getting dumber faster\n",
      "The fatter you fall behind\n",
      "Tripping on saggy tits\n",
      "Protein diet and dick\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "[Sometimes words can not say what the heart feels. Music can express the things that our language cannot.]\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "[I.]\n",
      "Se kaatui eteen\n",
      "Kdet kastoin vereen\n",
      "Se kaatui eteen\n",
      "Vain kuollut vapaa\n",
      "Slist sen\n",
      "Ihmislihan\n",
      "[II.]\n",
      "\n",
      "Genre: Hip-Hop\n",
      "Song:\n",
      "(Woo)\n",
      "(Chorus)\n",
      "(Baby) baby baby baby\n",
      "Is this something new? (something new)\n",
      "Or is this a bad case of d\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "The gratification of the ego\n",
      "Oge eht fo noitacifitarg eht\n",
      "The gratification of the ego\n",
      "Oge eht fo noitacifitarg eht\n",
      "\n",
      "Genre: Metal\n",
      "Song:\n",
      "[I.]\n",
      "Kun vieras jakaa\n",
      "Harhojaan\n",
      "Rukoillen myy vapauttaan\n",
      "Saasta maatukoon\n",
      "Srkykn henki\n",
      "Multaan tukehtuu harhojen renki\n",
      "[II.]\n",
      "\n",
      "Genre: Pop\n",
      "Song:\n",
      "Some people say: \"I would love to be strong\"\n",
      "Some people say: \"I do everything wrong\"\n",
      "Some people say: \"It\n",
      "\n",
      "Genre: Rock\n",
      "Song:\n",
      "We are not in a position to display these lyrics due to licensing restrictions. Sorry for the inconvinience.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, row in lyrics_df[lyrics_df.lyrics.apply(str.split).apply(len) <=20].iterrows():\n",
    "    print('Genre:',row.genre)\n",
    "    print('Song:')\n",
    "    print(row.lyrics)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YY7FFtktLI_6"
   },
   "outputs": [],
   "source": [
    "lyrics_df = lyrics_df[lyrics_df.lyrics != 'We are not in a position to display these lyrics due to licensing restrictions. Sorry for the inconvinience.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8TSck69LJAJ"
   },
   "source": [
    "## RNN for Text Generation\n",
    "In this section, we'll use an LSTM to generate new songs. You can pick any genre you like, or just use all genres. You can even try to generate songs in the style of a certain artist - remember that the Metrolyrics dataset contains the author of each song. \n",
    "\n",
    "For this, we’ll first train a character-based language model. We’ve mostly discussed in class the usage of RNNs to predict the next word given past words, but as we’ve mentioned in class, RNNs can also be used to learn sequences of characters.\n",
    "\n",
    "First, please go through the [PyTorch tutorial](https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html) on generating family names. You can download a .py file or a jupyter notebook with the entire code of the tutorial. \n",
    "\n",
    "As a reminder of topics we've discussed in class, see Andrej Karpathy's popular blog post [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](http://karpathy.github.io/2015/05/21/rnn-effectiveness/). You are also encouraged to view [this](https://gist.github.com/karpathy/d4dee566867f8291f086) vanilla implementation of a character-level RNN, written in numpy with just 100 lines of code, including the forward and backward passes.  \n",
    "\n",
    "Other tutorials that might prove useful:\n",
    "1. http://warmspringwinds.github.io/pytorch/rnns/2018/01/27/learning-to-generate-lyrics-and-music-with-recurrent-neural-networks/\n",
    "1. https://github.com/mcleonard/pytorch-charRNN\n",
    "1. https://github.com/spro/practical-pytorch/blob/master/char-rnn-generation/char-rnn-generation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNf_Quq4cyFR"
   },
   "outputs": [],
   "source": [
    "text = ' '.join(lyrics_df[(lyrics_df.genre == 'Hip-Hop')&(lyrics_df.artist == 'eminem')].lyrics.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "DGylSLEutsVc",
    "outputId": "0680d232-83ee-40e2-8b38-003989631b6e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-0d328bcf-b0fd-49b6-a4b6-26f5baa3a847\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>209.000000</td>\n",
       "      <td>209.000000</td>\n",
       "      <td>209.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2008.004785</td>\n",
       "      <td>3496.665072</td>\n",
       "      <td>2291.588517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.075176</td>\n",
       "      <td>1731.918473</td>\n",
       "      <td>1123.204581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2002.000000</td>\n",
       "      <td>276.000000</td>\n",
       "      <td>145.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2006.000000</td>\n",
       "      <td>2527.000000</td>\n",
       "      <td>1657.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2007.000000</td>\n",
       "      <td>3755.000000</td>\n",
       "      <td>2435.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2010.000000</td>\n",
       "      <td>4449.000000</td>\n",
       "      <td>2942.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2016.000000</td>\n",
       "      <td>11996.000000</td>\n",
       "      <td>6670.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d328bcf-b0fd-49b6-a4b6-26f5baa3a847')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-0d328bcf-b0fd-49b6-a4b6-26f5baa3a847 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-0d328bcf-b0fd-49b6-a4b6-26f5baa3a847');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "              year     num_chars    num_words\n",
       "count   209.000000    209.000000   209.000000\n",
       "mean   2008.004785   3496.665072  2291.588517\n",
       "std       3.075176   1731.918473  1123.204581\n",
       "min    2002.000000    276.000000   145.000000\n",
       "25%    2006.000000   2527.000000  1657.000000\n",
       "50%    2007.000000   3755.000000  2435.000000\n",
       "75%    2010.000000   4449.000000  2942.000000\n",
       "max    2016.000000  11996.000000  6670.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics_df[(lyrics_df.genre == 'Hip-Hop')&(lyrics_df.artist == 'eminem')].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qauU0LzaLJAJ"
   },
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVdwn20qLJAK"
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRy9CDl3LJAK"
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "az3oH_sqLJAK"
   },
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.contiguous().view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYsBK1z6LJAK"
   },
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Traing a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y)\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "           \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    with torch.no_grad():\n",
    "                        val_h = tuple(val_h)\n",
    "                        inputs, targets = x, y\n",
    "                        if cuda:\n",
    "                            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                        output, val_h = net.forward(inputs, val_h)\n",
    "                        val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                        val_losses.append(val_loss.item())\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzygvXjgLJAL"
   },
   "outputs": [],
   "source": [
    "if 'net' in locals():\n",
    "    del net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DASK_LhbLJAM"
   },
   "outputs": [],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXvV_DmrKuQT"
   },
   "source": [
    "First, we will try to run the model where each batch contains 128 sequences with  100 chars in each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cxcsiPKRLJAM",
    "outputId": "91b15d6c-8eda-4d48-c167-6ac613ddd19f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Step: 10... Loss: 3.4113... Val Loss: 3.3921\n",
      "Epoch: 1/10... Step: 20... Loss: 3.2964... Val Loss: 3.3042\n",
      "Epoch: 1/10... Step: 30... Loss: 3.1943... Val Loss: 3.2047\n",
      "Epoch: 1/10... Step: 40... Loss: 3.0721... Val Loss: 3.0679\n",
      "Epoch: 1/10... Step: 50... Loss: 2.9469... Val Loss: 2.9047\n",
      "Epoch: 2/10... Step: 60... Loss: 2.7227... Val Loss: 2.7538\n",
      "Epoch: 2/10... Step: 70... Loss: 2.6298... Val Loss: 2.6556\n",
      "Epoch: 2/10... Step: 80... Loss: 2.5516... Val Loss: 2.5856\n",
      "Epoch: 2/10... Step: 90... Loss: 2.4924... Val Loss: 2.5293\n",
      "Epoch: 2/10... Step: 100... Loss: 2.4820... Val Loss: 2.4851\n",
      "Epoch: 3/10... Step: 110... Loss: 2.4042... Val Loss: 2.4468\n",
      "Epoch: 3/10... Step: 120... Loss: 2.3679... Val Loss: 2.4220\n",
      "Epoch: 3/10... Step: 130... Loss: 2.3676... Val Loss: 2.3941\n",
      "Epoch: 3/10... Step: 140... Loss: 2.3053... Val Loss: 2.3718\n",
      "Epoch: 3/10... Step: 150... Loss: 2.2994... Val Loss: 2.3451\n",
      "Epoch: 4/10... Step: 160... Loss: 2.3205... Val Loss: 2.3236\n",
      "Epoch: 4/10... Step: 170... Loss: 2.2758... Val Loss: 2.3029\n",
      "Epoch: 4/10... Step: 180... Loss: 2.2393... Val Loss: 2.2873\n",
      "Epoch: 4/10... Step: 190... Loss: 2.2282... Val Loss: 2.2688\n",
      "Epoch: 4/10... Step: 200... Loss: 2.1528... Val Loss: 2.2512\n",
      "Epoch: 5/10... Step: 210... Loss: 2.1944... Val Loss: 2.2279\n",
      "Epoch: 5/10... Step: 220... Loss: 2.1675... Val Loss: 2.2179\n",
      "Epoch: 5/10... Step: 230... Loss: 2.1397... Val Loss: 2.2013\n",
      "Epoch: 5/10... Step: 240... Loss: 2.0960... Val Loss: 2.1862\n",
      "Epoch: 5/10... Step: 250... Loss: 2.1362... Val Loss: 2.1759\n",
      "Epoch: 6/10... Step: 260... Loss: 2.1614... Val Loss: 2.1715\n",
      "Epoch: 6/10... Step: 270... Loss: 2.1075... Val Loss: 2.1598\n",
      "Epoch: 6/10... Step: 280... Loss: 2.0619... Val Loss: 2.1434\n",
      "Epoch: 6/10... Step: 290... Loss: 2.1008... Val Loss: 2.1333\n",
      "Epoch: 6/10... Step: 300... Loss: 2.0679... Val Loss: 2.1229\n",
      "Epoch: 7/10... Step: 310... Loss: 2.0413... Val Loss: 2.1098\n",
      "Epoch: 7/10... Step: 320... Loss: 2.0120... Val Loss: 2.1005\n",
      "Epoch: 7/10... Step: 330... Loss: 2.0317... Val Loss: 2.0959\n",
      "Epoch: 7/10... Step: 340... Loss: 2.0421... Val Loss: 2.0825\n",
      "Epoch: 7/10... Step: 350... Loss: 2.0002... Val Loss: 2.0777\n",
      "Epoch: 8/10... Step: 360... Loss: 2.0212... Val Loss: 2.0631\n",
      "Epoch: 8/10... Step: 370... Loss: 2.0161... Val Loss: 2.0627\n",
      "Epoch: 8/10... Step: 380... Loss: 1.9629... Val Loss: 2.0506\n",
      "Epoch: 8/10... Step: 390... Loss: 1.9640... Val Loss: 2.0364\n",
      "Epoch: 8/10... Step: 400... Loss: 1.9115... Val Loss: 2.0384\n",
      "Epoch: 9/10... Step: 410... Loss: 1.9652... Val Loss: 2.0262\n",
      "Epoch: 9/10... Step: 420... Loss: 1.9603... Val Loss: 2.0204\n",
      "Epoch: 9/10... Step: 430... Loss: 1.9127... Val Loss: 2.0101\n",
      "Epoch: 9/10... Step: 440... Loss: 1.9154... Val Loss: 2.0054\n",
      "Epoch: 9/10... Step: 450... Loss: 1.8888... Val Loss: 2.0056\n",
      "Epoch: 10/10... Step: 460... Loss: 1.9282... Val Loss: 1.9972\n",
      "Epoch: 10/10... Step: 470... Loss: 1.9035... Val Loss: 1.9864\n",
      "Epoch: 10/10... Step: 480... Loss: 1.8818... Val Loss: 1.9823\n",
      "Epoch: 10/10... Step: 490... Loss: 1.8602... Val Loss: 1.9777\n",
      "Epoch: 10/10... Step: 500... Loss: 1.8868... Val Loss: 1.9651\n",
      "Epoch: 10/10... Step: 510... Loss: 1.9107... Val Loss: 1.9751\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=10, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=False, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ajYr2tzALJAM"
   },
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtvFBBbkLJAN"
   },
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yMhPDsHoLJAN",
    "outputId": "5744783d-f73c-4324-aefa-0b66b0a29888"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shit\n",
      "The cols the back in the crubber the my alman that the whot they that stimes over my fram\n",
      "I conl she mound a cauld of, then that and where the beet to take the fick is the stop me to say\n",
      "So when the shit any held of you the shoted make you and to make your bother as mither the beat a bund as throbe, shere and shat they shees what is shat me whone would your all the for this shown they when you say my newer that mosh the cars\n",
      "But you daint and motherfullin' the bettire and so be me,, I'm sicking,\n",
      "And it's somesting to get throut the merstict\n",
      "Somethere ain't the site to the from or the bord of thats and shit shinger all throw wolld and the shit op ond make, suck it's all ond it\n",
      "We triell the sticking to some ald mind to sem it\n",
      "Treeste fag on my nead then ain't out the stricked it\n",
      "I drestallin' the sonce and threw mess a bordes\n",
      "That's what whill the say it this bad i the wannats in the sang\n",
      "Trink, that who's way then sould\n",
      "I stape its this its that's a bat a be the thile\n",
      "Sleamy\n",
      "And what's a stand my sheel that shucking a butc me and thes way to trige my bod and throw me\n",
      "And it's gonna stirt that what\n",
      "I give and you dong mitherest on this world on topion out on the beckict is the meath\n",
      "I'm a sald the willes and thous in the came, some\n",
      "I'm thas all your a bother for a freems that's and you do me thatsion to through the fach the bathing to be on more my same and so fuckin' so lefe in the stills one on the sorn to me and this shat what that I get it's shit is and\n",
      "It's shit and shat in out trook\n",
      "And I'm said off in the same\n",
      "I getter mand out to be me on yin ain't try say my\n",
      "Bollin' all an the funking of me to get tick\n",
      "Shady all a fucking to stick my some\n",
      "And they wond the mome that I din't so dencare\n",
      "I'll be you showe than a cause they cliss without\n",
      "And you this if your theye to bady it's a bord the weres the mings to tell out of my back, what I strat in the say that man, I'm some and I'm liking that there make me\n",
      "I wenten to see\n",
      "I wand a can and a mothen\n",
      "I con at it it\n",
      "I'm\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='shit', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFQmosvZLJAN"
   },
   "source": [
    "Let’s test if the generated lyrics would make more sense when we will run various combinations of  “n_steps” and “n_seqs”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ugQvn_FUQ24o",
    "outputId": "537b86da-761d-42f0-d08c-8084d26cc872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "n_steps: 800 n_seqs: 10 num of batches: 91.376375\n",
      "Epoch: 1/10... Step: 10... Loss: 3.3793... Val Loss: 3.4060\n",
      "Epoch: 1/10... Step: 20... Loss: 3.2805... Val Loss: 3.2992\n",
      "Epoch: 1/10... Step: 30... Loss: 3.2666... Val Loss: 3.2005\n",
      "Epoch: 1/10... Step: 40... Loss: 3.0838... Val Loss: 3.0735\n",
      "Epoch: 1/10... Step: 50... Loss: 2.9298... Val Loss: 2.9262\n",
      "Epoch: 1/10... Step: 60... Loss: 2.8089... Val Loss: 2.7752\n",
      "Epoch: 1/10... Step: 70... Loss: 2.7246... Val Loss: 2.6864\n",
      "Epoch: 1/10... Step: 80... Loss: 2.6161... Val Loss: 2.6210\n",
      "Epoch: 2/10... Step: 90... Loss: 2.5502... Val Loss: 2.5627\n",
      "Epoch: 2/10... Step: 100... Loss: 2.5197... Val Loss: 2.5179\n",
      "Epoch: 2/10... Step: 110... Loss: 2.4727... Val Loss: 2.4757\n",
      "Epoch: 2/10... Step: 120... Loss: 2.4341... Val Loss: 2.4475\n",
      "Epoch: 2/10... Step: 130... Loss: 2.4762... Val Loss: 2.4155\n",
      "Epoch: 2/10... Step: 140... Loss: 2.4625... Val Loss: 2.3895\n",
      "Epoch: 2/10... Step: 150... Loss: 2.3601... Val Loss: 2.3583\n",
      "Epoch: 2/10... Step: 160... Loss: 2.3612... Val Loss: 2.3443\n",
      "Epoch: 3/10... Step: 170... Loss: 2.3006... Val Loss: 2.3326\n",
      "Epoch: 3/10... Step: 180... Loss: 2.2022... Val Loss: 2.3057\n",
      "Epoch: 3/10... Step: 190... Loss: 2.2375... Val Loss: 2.2885\n",
      "Epoch: 3/10... Step: 200... Loss: 2.2677... Val Loss: 2.2754\n",
      "Epoch: 3/10... Step: 210... Loss: 2.3308... Val Loss: 2.2619\n",
      "Epoch: 3/10... Step: 220... Loss: 2.2352... Val Loss: 2.2464\n",
      "Epoch: 3/10... Step: 230... Loss: 2.2650... Val Loss: 2.2241\n",
      "Epoch: 3/10... Step: 240... Loss: 2.2368... Val Loss: 2.2207\n",
      "Epoch: 4/10... Step: 250... Loss: 2.1409... Val Loss: 2.2076\n",
      "Epoch: 4/10... Step: 260... Loss: 2.1528... Val Loss: 2.1906\n",
      "Epoch: 4/10... Step: 270... Loss: 2.0970... Val Loss: 2.1767\n",
      "Epoch: 4/10... Step: 280... Loss: 2.1669... Val Loss: 2.1672\n",
      "Epoch: 4/10... Step: 290... Loss: 2.0957... Val Loss: 2.1624\n",
      "Epoch: 4/10... Step: 300... Loss: 2.1377... Val Loss: 2.1400\n",
      "Epoch: 4/10... Step: 310... Loss: 2.0085... Val Loss: 2.1294\n",
      "Epoch: 4/10... Step: 320... Loss: 2.0341... Val Loss: 2.1170\n",
      "Epoch: 5/10... Step: 330... Loss: 2.0865... Val Loss: 2.1147\n",
      "Epoch: 5/10... Step: 340... Loss: 2.0676... Val Loss: 2.1026\n",
      "Epoch: 5/10... Step: 350... Loss: 2.0592... Val Loss: 2.0945\n",
      "Epoch: 5/10... Step: 360... Loss: 2.0137... Val Loss: 2.0841\n",
      "Epoch: 5/10... Step: 370... Loss: 2.1129... Val Loss: 2.0820\n",
      "Epoch: 5/10... Step: 380... Loss: 2.0312... Val Loss: 2.0726\n",
      "Epoch: 5/10... Step: 390... Loss: 1.9572... Val Loss: 2.0588\n",
      "Epoch: 5/10... Step: 400... Loss: 2.0099... Val Loss: 2.0527\n",
      "Epoch: 5/10... Step: 410... Loss: 2.0266... Val Loss: 2.0548\n",
      "Epoch: 6/10... Step: 420... Loss: 2.0588... Val Loss: 2.0478\n",
      "Epoch: 6/10... Step: 430... Loss: 1.9403... Val Loss: 2.0423\n",
      "Epoch: 6/10... Step: 440... Loss: 2.0412... Val Loss: 2.0297\n",
      "Epoch: 6/10... Step: 450... Loss: 2.0321... Val Loss: 2.0208\n",
      "Epoch: 6/10... Step: 460... Loss: 1.9918... Val Loss: 2.0176\n",
      "Epoch: 6/10... Step: 470... Loss: 1.9707... Val Loss: 2.0061\n",
      "Epoch: 6/10... Step: 480... Loss: 2.0515... Val Loss: 1.9995\n",
      "Epoch: 6/10... Step: 490... Loss: 1.9604... Val Loss: 1.9951\n",
      "Epoch: 7/10... Step: 500... Loss: 1.9262... Val Loss: 2.0103\n",
      "Epoch: 7/10... Step: 510... Loss: 1.9503... Val Loss: 1.9928\n",
      "Epoch: 7/10... Step: 520... Loss: 1.9214... Val Loss: 1.9835\n",
      "Epoch: 7/10... Step: 530... Loss: 1.9015... Val Loss: 1.9840\n",
      "Epoch: 7/10... Step: 540... Loss: 2.0475... Val Loss: 1.9729\n",
      "Epoch: 7/10... Step: 550... Loss: 2.0120... Val Loss: 1.9638\n",
      "Epoch: 7/10... Step: 560... Loss: 1.9329... Val Loss: 1.9491\n",
      "Epoch: 7/10... Step: 570... Loss: 1.9446... Val Loss: 1.9509\n",
      "Epoch: 8/10... Step: 580... Loss: 1.8620... Val Loss: 1.9604\n",
      "Epoch: 8/10... Step: 590... Loss: 1.8039... Val Loss: 1.9452\n",
      "Epoch: 8/10... Step: 600... Loss: 1.8382... Val Loss: 1.9446\n",
      "Epoch: 8/10... Step: 610... Loss: 1.9047... Val Loss: 1.9389\n",
      "Epoch: 8/10... Step: 620... Loss: 1.9720... Val Loss: 1.9384\n",
      "Epoch: 8/10... Step: 630... Loss: 1.8979... Val Loss: 1.9272\n",
      "Epoch: 8/10... Step: 640... Loss: 1.9154... Val Loss: 1.9190\n",
      "Epoch: 8/10... Step: 650... Loss: 1.8910... Val Loss: 1.9187\n",
      "Epoch: 9/10... Step: 660... Loss: 1.8099... Val Loss: 1.9173\n",
      "Epoch: 9/10... Step: 670... Loss: 1.8612... Val Loss: 1.9164\n",
      "Epoch: 9/10... Step: 680... Loss: 1.8009... Val Loss: 1.9137\n",
      "Epoch: 9/10... Step: 690... Loss: 1.8497... Val Loss: 1.8990\n",
      "Epoch: 9/10... Step: 700... Loss: 1.8299... Val Loss: 1.9071\n",
      "Epoch: 9/10... Step: 710... Loss: 1.8675... Val Loss: 1.8963\n",
      "Epoch: 9/10... Step: 720... Loss: 1.7271... Val Loss: 1.8898\n",
      "Epoch: 9/10... Step: 730... Loss: 1.7830... Val Loss: 1.8901\n",
      "Epoch: 10/10... Step: 740... Loss: 1.8352... Val Loss: 1.8879\n",
      "Epoch: 10/10... Step: 750... Loss: 1.8549... Val Loss: 1.8991\n",
      "Epoch: 10/10... Step: 760... Loss: 1.8262... Val Loss: 1.8884\n",
      "Epoch: 10/10... Step: 770... Loss: 1.7471... Val Loss: 1.8829\n",
      "Epoch: 10/10... Step: 780... Loss: 1.8602... Val Loss: 1.8802\n",
      "Epoch: 10/10... Step: 790... Loss: 1.8224... Val Loss: 1.8778\n",
      "Epoch: 10/10... Step: 800... Loss: 1.7442... Val Loss: 1.8678\n",
      "Epoch: 10/10... Step: 810... Loss: 1.8065... Val Loss: 1.8639\n",
      "Epoch: 10/10... Step: 820... Loss: 1.7625... Val Loss: 1.8658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shit to mys, stick off alm sime sing my bedy say man as\n",
      "I'm say my sick an my shit is the fent, they, I camed it's so but I was startin mar ands that a fucked and was the show mine on a carse a care take to stall a been in in a canstend,\n",
      "I'm time in\n",
      "They stray want you a mant\n",
      "I was dence my niggas the wayn't would wanna shat in your foolle we and staped the closh\n",
      "I'm to saight\n",
      "I said I am that's a streed man, with a botthes back and that\n",
      "Said, I'm trying a munne the with threst's and my car sack\n",
      "Talk year,\n",
      "I'm sing thes to the the blat its\n",
      "If to tell handed me\n",
      "Trust this shit to be show the same atsent my backs that I'm stilling me\n",
      "All I got a lot one sitches\n",
      "Whel you said I can be the botceres to be the back and shit and the wonten they stread of man and you couse\n",
      "They so she was the sead, wanna get it all that\n",
      "I'm tired, stillin shat that as moth mathere and send, to the cancer a sellart take me they careded my niggas, the mest they white your tasters and me\n",
      "I'm same still of you cause I dent someof you was and that and my clance\n",
      "I'm the so dough, the shand with the say\n",
      "And they want is my bitcouse in the man an mace and ya\n",
      "You said\n",
      "You did that I'm tried a cang the said\n",
      "I'm a fuckin' bust\n",
      "And I come be to make the bods\n",
      "And you gay a man and the bast say and strep beang sord and and you so bother song shit\n",
      "Stary to get me the bas on the cance and you any all the mach on you told shout me\n",
      "And you know you can talk is they can chang thing the frint all they cond on my bell ant sinnow they sand the would sain its somethat who whe way yeah that I day and I wonld to man\n",
      "A bay I'm a bat went at me and I want a shat\n",
      "So y's call a lestle tired of the same on a coulde arount, you come stop a beath the whater to collay\n",
      "I want a melled there was shit, at me and I was a chest on my mond and tryin through than's thrust to take the wand all the baby a dad what the whole ast\n",
      "It songs and the creal of\n",
      "Tried you and some and I'll trat of a strep try my bad,\n",
      "Shid it the baby as the buck\n",
      "===============================================\n",
      "n_steps: 150 n_seqs: 50 num of batches: 97.46813333333333\n",
      "Epoch: 1/10... Step: 10... Loss: 3.3951... Val Loss: 3.3961\n",
      "Epoch: 1/10... Step: 20... Loss: 3.2950... Val Loss: 3.2867\n",
      "Epoch: 1/10... Step: 30... Loss: 3.1771... Val Loss: 3.1815\n",
      "Epoch: 1/10... Step: 40... Loss: 3.0783... Val Loss: 3.0409\n",
      "Epoch: 1/10... Step: 50... Loss: 2.8265... Val Loss: 2.8701\n",
      "Epoch: 1/10... Step: 60... Loss: 2.7499... Val Loss: 2.7463\n",
      "Epoch: 1/10... Step: 70... Loss: 2.6351... Val Loss: 2.6607\n",
      "Epoch: 1/10... Step: 80... Loss: 2.5621... Val Loss: 2.5924\n",
      "Epoch: 2/10... Step: 90... Loss: 2.5051... Val Loss: 2.5348\n",
      "Epoch: 2/10... Step: 100... Loss: 2.4391... Val Loss: 2.4960\n",
      "Epoch: 2/10... Step: 110... Loss: 2.4315... Val Loss: 2.4610\n",
      "Epoch: 2/10... Step: 120... Loss: 2.3793... Val Loss: 2.4251\n",
      "Epoch: 2/10... Step: 130... Loss: 2.3406... Val Loss: 2.4076\n",
      "Epoch: 2/10... Step: 140... Loss: 2.4037... Val Loss: 2.3817\n",
      "Epoch: 2/10... Step: 150... Loss: 2.2989... Val Loss: 2.3565\n",
      "Epoch: 2/10... Step: 160... Loss: 2.3048... Val Loss: 2.3352\n",
      "Epoch: 2/10... Step: 170... Loss: 2.2892... Val Loss: 2.3189\n",
      "Epoch: 3/10... Step: 180... Loss: 2.2686... Val Loss: 2.2962\n",
      "Epoch: 3/10... Step: 190... Loss: 2.2278... Val Loss: 2.2839\n",
      "Epoch: 3/10... Step: 200... Loss: 2.2090... Val Loss: 2.2656\n",
      "Epoch: 3/10... Step: 210... Loss: 2.2224... Val Loss: 2.2491\n",
      "Epoch: 3/10... Step: 220... Loss: 2.2052... Val Loss: 2.2383\n",
      "Epoch: 3/10... Step: 230... Loss: 2.1977... Val Loss: 2.2226\n",
      "Epoch: 3/10... Step: 240... Loss: 2.1880... Val Loss: 2.2094\n",
      "Epoch: 3/10... Step: 250... Loss: 2.1840... Val Loss: 2.1954\n",
      "Epoch: 3/10... Step: 260... Loss: 2.1239... Val Loss: 2.1842\n",
      "Epoch: 4/10... Step: 270... Loss: 2.1516... Val Loss: 2.1761\n",
      "Epoch: 4/10... Step: 280... Loss: 2.1198... Val Loss: 2.1678\n",
      "Epoch: 4/10... Step: 290... Loss: 2.0745... Val Loss: 2.1549\n",
      "Epoch: 4/10... Step: 300... Loss: 2.0480... Val Loss: 2.1464\n",
      "Epoch: 4/10... Step: 310... Loss: 2.0632... Val Loss: 2.1303\n",
      "Epoch: 4/10... Step: 320... Loss: 2.1493... Val Loss: 2.1222\n",
      "Epoch: 4/10... Step: 330... Loss: 2.0556... Val Loss: 2.1104\n",
      "Epoch: 4/10... Step: 340... Loss: 2.0439... Val Loss: 2.1054\n",
      "Epoch: 5/10... Step: 350... Loss: 2.0737... Val Loss: 2.0900\n",
      "Epoch: 5/10... Step: 360... Loss: 2.0398... Val Loss: 2.0848\n",
      "Epoch: 5/10... Step: 370... Loss: 2.0420... Val Loss: 2.0787\n",
      "Epoch: 5/10... Step: 380... Loss: 1.9618... Val Loss: 2.0714\n"
     ]
    }
   ],
   "source": [
    "n_seqs_list=[10,50,150,800] \n",
    "n_steps_list=[10,50,150,800] \n",
    "max_batches=100\n",
    "min_batches=30\n",
    "\n",
    "for n_seqs in n_seqs_list:\n",
    "  for n_steps in n_steps_list:\n",
    "    n_b=len(encoded)/(n_seqs*n_steps)\n",
    "    if n_b<max_batches and n_b>min_batches:\n",
    "      print(\"===============================================\")\n",
    "      print(\"n_steps:\", n_steps, \"n_seqs:\", n_seqs, \"num of batches:\",n_b )\n",
    "      if 'net' in locals():\n",
    "        del net\n",
    "      net = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "      train(net, encoded, epochs=10, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=False, print_every=10)\n",
    "      print(sample(net, 2000, prime='shit', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4NZbEGZCD5L8",
    "outputId": "ab798c07-17f4-4421-9f49-9aaa1704c748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================\n",
      "n_steps: 150 n_seqs: 80 num of batches: 60.91758333333333\n",
      "Epoch: 1/10... Step: 10... Loss: 3.4402... Val Loss: 3.4057\n",
      "Epoch: 1/10... Step: 20... Loss: 3.2769... Val Loss: 3.3135\n",
      "Epoch: 1/10... Step: 30... Loss: 3.1819... Val Loss: 3.2024\n",
      "Epoch: 1/10... Step: 40... Loss: 3.0904... Val Loss: 3.0681\n",
      "Epoch: 1/10... Step: 50... Loss: 2.9182... Val Loss: 2.9086\n",
      "Epoch: 2/10... Step: 60... Loss: 2.7622... Val Loss: 2.7550\n",
      "Epoch: 2/10... Step: 70... Loss: 2.6872... Val Loss: 2.6617\n",
      "Epoch: 2/10... Step: 80... Loss: 2.5986... Val Loss: 2.5922\n",
      "Epoch: 2/10... Step: 90... Loss: 2.4930... Val Loss: 2.5361\n",
      "Epoch: 2/10... Step: 100... Loss: 2.4681... Val Loss: 2.4873\n",
      "Epoch: 3/10... Step: 110... Loss: 2.4258... Val Loss: 2.4430\n",
      "Epoch: 3/10... Step: 120... Loss: 2.3861... Val Loss: 2.4104\n",
      "Epoch: 3/10... Step: 130... Loss: 2.3447... Val Loss: 2.3881\n",
      "Epoch: 3/10... Step: 140... Loss: 2.3435... Val Loss: 2.3682\n",
      "Epoch: 3/10... Step: 150... Loss: 2.3370... Val Loss: 2.3462\n",
      "Epoch: 3/10... Step: 160... Loss: 2.2874... Val Loss: 2.3184\n",
      "Epoch: 4/10... Step: 170... Loss: 2.2936... Val Loss: 2.2896\n",
      "Epoch: 4/10... Step: 180... Loss: 2.2669... Val Loss: 2.2737\n",
      "Epoch: 4/10... Step: 190... Loss: 2.2158... Val Loss: 2.2582\n",
      "Epoch: 4/10... Step: 200... Loss: 2.1941... Val Loss: 2.2376\n",
      "Epoch: 4/10... Step: 210... Loss: 2.1549... Val Loss: 2.2205\n",
      "Epoch: 5/10... Step: 220... Loss: 2.1899... Val Loss: 2.2023\n",
      "Epoch: 5/10... Step: 230... Loss: 2.1618... Val Loss: 2.1918\n",
      "Epoch: 5/10... Step: 240... Loss: 2.1132... Val Loss: 2.1760\n",
      "Epoch: 5/10... Step: 250... Loss: 2.0961... Val Loss: 2.1615\n",
      "Epoch: 5/10... Step: 260... Loss: 2.1440... Val Loss: 2.1601\n",
      "Epoch: 5/10... Step: 270... Loss: 2.1309... Val Loss: 2.1420\n",
      "Epoch: 6/10... Step: 280... Loss: 2.1063... Val Loss: 2.1318\n",
      "Epoch: 6/10... Step: 290... Loss: 2.0438... Val Loss: 2.1145\n",
      "Epoch: 6/10... Step: 300... Loss: 2.0489... Val Loss: 2.1053\n",
      "Epoch: 6/10... Step: 310... Loss: 2.0422... Val Loss: 2.0948\n",
      "Epoch: 6/10... Step: 320... Loss: 2.0462... Val Loss: 2.0854\n",
      "Epoch: 7/10... Step: 330... Loss: 2.0494... Val Loss: 2.0718\n",
      "Epoch: 7/10... Step: 340... Loss: 2.0540... Val Loss: 2.0629\n",
      "Epoch: 7/10... Step: 350... Loss: 2.0272... Val Loss: 2.0546\n",
      "Epoch: 7/10... Step: 360... Loss: 1.9691... Val Loss: 2.0462\n",
      "Epoch: 7/10... Step: 370... Loss: 1.9853... Val Loss: 2.0418\n",
      "Epoch: 8/10... Step: 380... Loss: 1.9632... Val Loss: 2.0255\n",
      "Epoch: 8/10... Step: 390... Loss: 1.9681... Val Loss: 2.0220\n",
      "Epoch: 8/10... Step: 400... Loss: 1.9562... Val Loss: 2.0155\n",
      "Epoch: 8/10... Step: 410... Loss: 1.9657... Val Loss: 2.0058\n",
      "Epoch: 8/10... Step: 420... Loss: 1.9802... Val Loss: 1.9998\n",
      "Epoch: 8/10... Step: 430... Loss: 1.9396... Val Loss: 1.9923\n",
      "Epoch: 9/10... Step: 440... Loss: 1.9800... Val Loss: 1.9847\n",
      "Epoch: 9/10... Step: 450... Loss: 1.9528... Val Loss: 1.9895\n",
      "Epoch: 9/10... Step: 460... Loss: 1.9189... Val Loss: 1.9798\n",
      "Epoch: 9/10... Step: 470... Loss: 1.8856... Val Loss: 1.9728\n",
      "Epoch: 9/10... Step: 480... Loss: 1.8953... Val Loss: 1.9671\n",
      "Epoch: 10/10... Step: 490... Loss: 1.9237... Val Loss: 1.9596\n",
      "Epoch: 10/10... Step: 500... Loss: 1.9112... Val Loss: 1.9504\n",
      "Epoch: 10/10... Step: 510... Loss: 1.8637... Val Loss: 1.9484\n",
      "Epoch: 10/10... Step: 520... Loss: 1.8389... Val Loss: 1.9468\n",
      "Epoch: 10/10... Step: 530... Loss: 1.8758... Val Loss: 1.9351\n",
      "Epoch: 10/10... Step: 540... Loss: 1.8746... Val Loss: 1.9259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:50: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shit out to to getto to them take i with a bass astal of\n",
      "I'm a can if the will stalkin' shat\n",
      "I'm lookin' this whole\n",
      "They's got me\n",
      "It's to see you to dambe i some it\n",
      "I don't want it's back with athar thess\n",
      "I'm stakid a filling bort wast out a fuckin' but I've sand me\n",
      "To the tay to my a sard the seen a can alling\n",
      "I con't shoutt to throw to sell\n",
      "I got's beady with them to the werly trick to stall word\n",
      "Wass that walkin' be the but I wont and I'm a crown a stirle on my stint an the sholle\n",
      "I'll be yaug you're all make that shad you're to bothing we go back\n",
      "I seep micking they sead off at mise my as\n",
      "As they's they go but he someto\n",
      "And I'm andin' one mars, woll the mar the fremin'\n",
      "Indo that we'll she we to so the seens and shat that the whole at me the mor the bock to mades, bothing thene shit on the well some in itser\n",
      "And your ast semather that I would with at\n",
      "Witches so live anound\n",
      "You wonted the wordd atton mo to she sand\n",
      "And your bett me in me is some to more\n",
      "We're shop they was somet and to gottart\n",
      "But in this a sone of my some\n",
      "And you can you don't got the soun and whe withor ass\n",
      "And you can thit it to be a fight me to bloth, then' at the mor that the whote\n",
      "We see that I way to the but to be our to so wish has to mak these trish\n",
      "I'm a chinca bott all stropsent to to me\n",
      "And I was a poodan that wher your hister indart\n",
      "You're back on the wonts all me\n",
      "Back on the short\n",
      "And you ain't seep to my all stice\n",
      "And it's all the wart our at on the canting my stome they sell on the mare in\n",
      "I'm tonna some\n",
      "I shit thes while I do there's a cance, I wouldn't get a mad a beat, hand I'm listil to hat on my sand\n",
      "So they say and where\n",
      "I shout tree at the wast my name\n",
      "You're a sing tricks and a bable the buck whers we sen you\n",
      "That I shere and you all I'm a from but I whough you don't say\n",
      "I we look it this is the but think a body, say I was steppin a binc\n",
      "I'm a firstaterssad as strack and we collers\n",
      "But tome they street in, I gon' but you can a be and the mat\n",
      "And I sain you shope a sentole to get\n",
      "===============================================\n",
      "n_steps: 80 n_seqs: 150 num of batches: 60.91758333333333\n",
      "Epoch: 1/10... Step: 10... Loss: 3.3715... Val Loss: 3.3901\n",
      "Epoch: 1/10... Step: 20... Loss: 3.2873... Val Loss: 3.2901\n",
      "Epoch: 1/10... Step: 30... Loss: 3.1535... Val Loss: 3.1785\n",
      "Epoch: 1/10... Step: 40... Loss: 3.0440... Val Loss: 3.0415\n",
      "Epoch: 1/10... Step: 50... Loss: 2.8853... Val Loss: 2.8719\n",
      "Epoch: 2/10... Step: 60... Loss: 2.7195... Val Loss: 2.7292\n",
      "Epoch: 2/10... Step: 70... Loss: 2.6139... Val Loss: 2.6338\n",
      "Epoch: 2/10... Step: 80... Loss: 2.5403... Val Loss: 2.5687\n",
      "Epoch: 2/10... Step: 90... Loss: 2.4797... Val Loss: 2.5134\n",
      "Epoch: 2/10... Step: 100... Loss: 2.4520... Val Loss: 2.4689\n",
      "Epoch: 3/10... Step: 110... Loss: 2.4014... Val Loss: 2.4313\n",
      "Epoch: 3/10... Step: 120... Loss: 2.3855... Val Loss: 2.4119\n",
      "Epoch: 3/10... Step: 130... Loss: 2.3540... Val Loss: 2.3765\n",
      "Epoch: 3/10... Step: 140... Loss: 2.3458... Val Loss: 2.3584\n",
      "Epoch: 3/10... Step: 150... Loss: 2.2902... Val Loss: 2.3329\n",
      "Epoch: 3/10... Step: 160... Loss: 2.2777... Val Loss: 2.3105\n",
      "Epoch: 4/10... Step: 170... Loss: 2.2314... Val Loss: 2.2862\n",
      "Epoch: 4/10... Step: 180... Loss: 2.2337... Val Loss: 2.2726\n",
      "Epoch: 4/10... Step: 190... Loss: 2.1976... Val Loss: 2.2553\n",
      "Epoch: 4/10... Step: 200... Loss: 2.1678... Val Loss: 2.2390\n",
      "Epoch: 4/10... Step: 210... Loss: 2.1792... Val Loss: 2.2215\n",
      "Epoch: 5/10... Step: 220... Loss: 2.1738... Val Loss: 2.2037\n",
      "Epoch: 5/10... Step: 230... Loss: 2.1434... Val Loss: 2.1904\n",
      "Epoch: 5/10... Step: 240... Loss: 2.1203... Val Loss: 2.1727\n",
      "Epoch: 5/10... Step: 250... Loss: 2.1123... Val Loss: 2.1641\n",
      "Epoch: 5/10... Step: 260... Loss: 2.1426... Val Loss: 2.1510\n",
      "Epoch: 5/10... Step: 270... Loss: 2.0823... Val Loss: 2.1371\n",
      "Epoch: 6/10... Step: 280... Loss: 2.0495... Val Loss: 2.1260\n",
      "Epoch: 6/10... Step: 290... Loss: 2.0700... Val Loss: 2.1118\n",
      "Epoch: 6/10... Step: 300... Loss: 2.0409... Val Loss: 2.0985\n",
      "Epoch: 6/10... Step: 310... Loss: 2.0226... Val Loss: 2.0912\n",
      "Epoch: 6/10... Step: 320... Loss: 2.0555... Val Loss: 2.0909\n",
      "Epoch: 7/10... Step: 330... Loss: 2.0384... Val Loss: 2.0663\n",
      "Epoch: 7/10... Step: 340... Loss: 1.9830... Val Loss: 2.0588\n",
      "Epoch: 7/10... Step: 350... Loss: 1.9835... Val Loss: 2.0475\n",
      "Epoch: 7/10... Step: 360... Loss: 1.9981... Val Loss: 2.0423\n",
      "Epoch: 7/10... Step: 370... Loss: 1.9747... Val Loss: 2.0336\n",
      "Epoch: 8/10... Step: 380... Loss: 1.9604... Val Loss: 2.0336\n",
      "Epoch: 8/10... Step: 390... Loss: 1.9544... Val Loss: 2.0155\n",
      "Epoch: 8/10... Step: 400... Loss: 1.9691... Val Loss: 2.0133\n",
      "Epoch: 8/10... Step: 410... Loss: 1.9658... Val Loss: 2.0131\n",
      "Epoch: 8/10... Step: 420... Loss: 1.9366... Val Loss: 2.0011\n",
      "Epoch: 8/10... Step: 430... Loss: 1.9171... Val Loss: 1.9914\n",
      "Epoch: 9/10... Step: 440... Loss: 1.8921... Val Loss: 1.9899\n",
      "Epoch: 9/10... Step: 450... Loss: 1.9123... Val Loss: 1.9777\n",
      "Epoch: 9/10... Step: 460... Loss: 1.8584... Val Loss: 1.9684\n",
      "Epoch: 9/10... Step: 470... Loss: 1.8408... Val Loss: 1.9675\n",
      "Epoch: 9/10... Step: 480... Loss: 1.8779... Val Loss: 1.9558\n",
      "Epoch: 10/10... Step: 490... Loss: 1.8987... Val Loss: 1.9487\n",
      "Epoch: 10/10... Step: 500... Loss: 1.8543... Val Loss: 1.9445\n",
      "Epoch: 10/10... Step: 510... Loss: 1.8545... Val Loss: 1.9377\n",
      "Epoch: 10/10... Step: 520... Loss: 1.8845... Val Loss: 1.9365\n",
      "Epoch: 10/10... Step: 530... Loss: 1.8913... Val Loss: 1.9294\n",
      "Epoch: 10/10... Step: 540... Loss: 1.8410... Val Loss: 1.9255\n",
      "shit the bad as me,\n",
      "I'll not to stoll out the bood\n",
      "So thin I get your than the cars somene shooted\n",
      "I'm lift you dong to the boon sould we drapped\n",
      "The stuck on that to sain that sounde to the peace sometel\n",
      "I'm get it, you too to go back, but I wanna startid you all, you did you this it allored, bitchin' think I deeped\n",
      "I world didn' the tome it tike my say\n",
      "I'm grablig and the wordd on mere foure\n",
      "Bitcher that I'm like a better but the coldids anomole, boush\n",
      "Show you a black in the cound shattated to she wanta be to tryin' tike a fuck this and\n",
      "All I was timed a combly bat you a sand of a little balls\n",
      "I can a bott me\n",
      "And when you call mane my nack on the woold tryin' that?\n",
      "Where's what the fame\n",
      "And you get this what I dope to talk the short, shake all atsed what\n",
      "The clab be a gong to get a bills\n",
      "I won't be in this somen the bangs is saintens\n",
      "I'm stantin' the bout\n",
      "I whon I was a sand a sencar that I'm sout of they stolled on yo\n",
      "\n",
      "I sond the fuck alone op strip on a blach\n",
      "To herough that, I'm say\n",
      "Tell the from shit on the cast and threan, you got a seet of you the whold and a crick off a shit at and trena trop back\n",
      "In a blabe thoule and there's beat\n",
      "I was don't fuck to but, at the cause I don't know why what I can shis wendy\n",
      "This that's shot white I do and ass tolded of your a fack all me take youre too the furnes\n",
      "I want want a crop the same and wat a cante\n",
      "Ther time to hear me, hold till I was tole in my same\n",
      "The fack out the what you cone\n",
      "That's while I do as me to been is to how it this, what throw you as my bick and that at the cance, this whe wanta be for than so the clast son't get me thought, I'm get it, you a sain a lott of mising\n",
      "Thostice the somenoted as and as and say\n",
      "To to get on of but band in me so be alound and that wishors wannt alled\n",
      "And you and a said the combones\n",
      "It's black as a little beat in the stop there shook of you bit hadd\n",
      "The stapp the some ander short\n",
      "I'm garna tires\n",
      "I'm sonding of the with me with mosher\n",
      "But as a fucking on than the fure is the talk \n",
      "===============================================\n",
      "n_steps: 150 n_seqs: 150 num of batches: 32.489377777777776\n",
      "Epoch: 1/10... Step: 10... Loss: 3.3563... Val Loss: 3.3572\n",
      "Epoch: 1/10... Step: 20... Loss: 3.2755... Val Loss: 3.2839\n",
      "Epoch: 2/10... Step: 30... Loss: 3.1964... Val Loss: 3.1719\n",
      "Epoch: 2/10... Step: 40... Loss: 3.0347... Val Loss: 3.0326\n",
      "Epoch: 2/10... Step: 50... Loss: 2.8440... Val Loss: 2.8502\n",
      "Epoch: 3/10... Step: 60... Loss: 2.7125... Val Loss: 2.7331\n",
      "Epoch: 3/10... Step: 70... Loss: 2.6169... Val Loss: 2.6438\n",
      "Epoch: 3/10... Step: 80... Loss: 2.5498... Val Loss: 2.5765\n",
      "Epoch: 4/10... Step: 90... Loss: 2.4711... Val Loss: 2.5149\n",
      "Epoch: 4/10... Step: 100... Loss: 2.4547... Val Loss: 2.4718\n",
      "Epoch: 4/10... Step: 110... Loss: 2.4039... Val Loss: 2.4357\n",
      "Epoch: 5/10... Step: 120... Loss: 2.3604... Val Loss: 2.4121\n",
      "Epoch: 5/10... Step: 130... Loss: 2.3461... Val Loss: 2.3766\n",
      "Epoch: 5/10... Step: 140... Loss: 2.3272... Val Loss: 2.3503\n",
      "Epoch: 6/10... Step: 150... Loss: 2.2781... Val Loss: 2.3298\n",
      "Epoch: 6/10... Step: 160... Loss: 2.2548... Val Loss: 2.3038\n",
      "Epoch: 6/10... Step: 170... Loss: 2.2700... Val Loss: 2.2838\n",
      "Epoch: 7/10... Step: 180... Loss: 2.2150... Val Loss: 2.2617\n",
      "Epoch: 7/10... Step: 190... Loss: 2.2164... Val Loss: 2.2425\n",
      "Epoch: 7/10... Step: 200... Loss: 2.1960... Val Loss: 2.2218\n",
      "Epoch: 8/10... Step: 210... Loss: 2.1723... Val Loss: 2.2061\n",
      "Epoch: 8/10... Step: 220... Loss: 2.1420... Val Loss: 2.1938\n",
      "Epoch: 8/10... Step: 230... Loss: 2.1237... Val Loss: 2.1752\n",
      "Epoch: 9/10... Step: 240... Loss: 2.1133... Val Loss: 2.1562\n",
      "Epoch: 9/10... Step: 250... Loss: 2.0884... Val Loss: 2.1463\n",
      "Epoch: 9/10... Step: 260... Loss: 2.0578... Val Loss: 2.1270\n",
      "Epoch: 10/10... Step: 270... Loss: 2.0638... Val Loss: 2.1174\n",
      "Epoch: 10/10... Step: 280... Loss: 2.0448... Val Loss: 2.1022\n",
      "Epoch: 10/10... Step: 290... Loss: 2.0429... Val Loss: 2.0887\n",
      "shit to car but in thass that sandy, wash ats\n",
      "The tay to bat on, bits thin you\n",
      "All they chick\n",
      "And I want you can the shis to this wourd and strow the warna\n",
      "I gond to so treanter and\n",
      "I'll nighin your me\n",
      "Yia seit, you ton they shit\n",
      "Showh tray a bat is\n",
      "All thil I can's give an that hare\n",
      "Sand you saik a bothang, this a sheth\n",
      "But I'm and the the what the share, the stake to hers try mantisting\n",
      "I caund to see a that al the same\n",
      "I get and the tryon you badin mo south\n",
      "But I don's got it of thit a cause foo in tor the stoper to so thit the saice off is\n",
      "I gring a fucking out\n",
      "I'm shore to steere that sell wist it\n",
      "Be for you\n",
      "You don't sool the sait that trat\n",
      "Thay his she wan you the stels an it's a comed this\n",
      "I'm the the facke to the foor the the fuckire and stace\n",
      "If I don't timing bet and to so me say to shit\n",
      "And you to to souck take\n",
      "I soin the the times alfol in that\n",
      "I'm an you to shile\n",
      "I want a shit and and me to bitch that a butches it time out if they wate same it\n",
      "Stin you ther sant to be and as is the that harse\n",
      "And a to bean time\n",
      "I'm becking in a frock all the wort\n",
      "And the sto till\n",
      "Thit I sont thes this whet the whing this wanct, I'll shit aint in trees\n",
      "I'm tonnt and thene\n",
      "I do stind you to bay bat\n",
      "And I wound to bick it's and in as to has in as the misher\n",
      "So you'rall so my sang the stol it\n",
      "Show the stort it the saintit the backing\n",
      "I'm leave as and the shat in that still me in me it it on\n",
      "Standy\n",
      "And I give it silled out to the won make me and the stere\n",
      "I gat throme\n",
      "I dad this that a corlass this was a manet of you tit\n",
      "So a that he the wase\n",
      "You cropledint the beat that standing one shat\n",
      "I gon a chitse it to selle\n",
      "I can alline shin tright, then tise it\n",
      "Stray hem on you aintat onted ous\n",
      "This shill andys in the bisted\n",
      "A tand one to buck the sack making\n",
      "I who dand your tha keas on me on man to be crouch in the selle is then a can to sere tile sit as\n",
      "I sapped to his on a bocken why I'm not is in it, you soot me ain' that al this some\n",
      "I'm buck the bettin't stick and as that and tare\n"
     ]
    }
   ],
   "source": [
    "n_seqs_list=[80,150] \n",
    "n_steps_list=[80,150] \n",
    "max_batches=100\n",
    "min_batches=30\n",
    "\n",
    "for n_seqs in n_seqs_list:\n",
    "  for n_steps in n_steps_list:\n",
    "    n_b=len(encoded)/(n_seqs*n_steps)\n",
    "    if n_b<max_batches and n_b>min_batches:\n",
    "      print(\"===============================================\")\n",
    "      print(\"n_steps:\", n_steps, \"n_seqs:\", n_seqs, \"num of batches:\",n_b )\n",
    "      if 'net' in locals():\n",
    "        del net\n",
    "      net = CharRNN(chars, n_hidden=512, n_layers=2)\n",
    "      train(net, encoded, epochs=10, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=False, print_every=10)\n",
    "      print(sample(net, 2000, prime='shit', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ8BJXNzG-EC"
   },
   "source": [
    "<b> *Comments:* <b> \n",
    "\n",
    "\n",
    "*   Tried to improve the model by testing different \"n_steps\" and \"n_seqs\", but it didn't really improve much the validation loss\n",
    "*  the model ran each time with 10 epochs, but it is almost certain that the text will be more logical with more epochs. the challange was the time it took to run the model\n",
    "* by the 10th epoch, some of the words are still not english, \n",
    "but we can see that the model learned a lot of profanity, which is characteristic of Eminem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rq--eosL7gm"
   },
   "source": [
    "### Final Tips\n",
    "\n",
    "As a final tip, we do encourage you to do most of the work first on your local machine. They say that Data Scientists spend 80% of their time cleaning the data and preparing it for training (and 20% complaining about cleaning the data and preparing it). Handling these parts on your local machine usually mean you will spend less time complaining. You can switch to the cloud once your code runs and your pipeline is in place, for the actual training using a GPU.\n",
    "\n",
    "We also encourage you to use a small subset of the dataset first, so things run smoothly. The Metrolyrics dataset contains over 300k songs. You can start with a much much smaller set (even 3,000 songs) and try to train a network based on it. Once everything runs properly, add more data.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOdJsjPd0vhI"
   },
   "source": [
    "#### This exericse was originally written by Dr. Omri Allouche."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "DL_rnn_generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
